{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SelfAttnCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6e/1T0UJ057g7BnIFbYc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b74fe9c00b5497592b6445edf2d89f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db9cf7b6fb334e81a0c1961ac073bf25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2d358d38753409b821e89320724b893",
              "IPY_MODEL_60d06d9624be437ea324aeb3c4aaac9e"
            ]
          }
        },
        "db9cf7b6fb334e81a0c1961ac073bf25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2d358d38753409b821e89320724b893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_57a6a1bbee714c82ac14b40dd3cca585",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 169001437,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 169001437,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cc1369ecab54d0ca4305cb15a139dd2"
          }
        },
        "60d06d9624be437ea324aeb3c4aaac9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb01dd081ab1465f890f744954dace40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169001984/? [00:07&lt;00:00, 23221651.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_122c359f6df8429ead902dde97b9d8f9"
          }
        },
        "57a6a1bbee714c82ac14b40dd3cca585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cc1369ecab54d0ca4305cb15a139dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb01dd081ab1465f890f744954dace40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "122c359f6df8429ead902dde97b9d8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertramp2/Colab/blob/main/SelfAttnCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVp4ygtM3Rr"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torchvision\n",
        "import torchvision.utils as utils\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EG4hac_FFN2"
      },
      "source": [
        "def _worker_init_fn_():\n",
        "    torch_seed = torch.initial_seed()\n",
        "    np_seed = torch_seed // 2**32-1\n",
        "    random.seed(torch_seed)\n",
        "    np.random.seed(np_seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNwqqh3fFFQ8"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfm7oFtq25vS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "9b74fe9c00b5497592b6445edf2d89f0",
            "db9cf7b6fb334e81a0c1961ac073bf25",
            "d2d358d38753409b821e89320724b893",
            "60d06d9624be437ea324aeb3c4aaac9e",
            "57a6a1bbee714c82ac14b40dd3cca585",
            "5cc1369ecab54d0ca4305cb15a139dd2",
            "eb01dd081ab1465f890f744954dace40",
            "122c359f6df8429ead902dde97b9d8f9"
          ]
        },
        "outputId": "083b4527-4dde-4e39-f803-8f3cd22053a1"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size= 64, shuffle=True, num_workers=2, worker_init_fn=_worker_init_fn_())\n",
        "testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to CIFAR100_data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b74fe9c00b5497592b6445edf2d89f0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting CIFAR100_data/cifar-100-python.tar.gz to CIFAR100_data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1MrdPUi_43e"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        features = [in_features] + [out_features for i in range(num_conv)]\n",
        "        layers = []\n",
        "        for i in range(len(features)-1):\n",
        "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU())\n",
        "            if pool:\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        self.op = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I18g3Vfh_5Db"
      },
      "source": [
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, inputs):\n",
        "        return self.op(inputs)\n",
        "        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmMT3in_5FK"
      },
      "source": [
        "class LinearAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features, normalize_attn=True):\n",
        "        super(LinearAttentionBlock, self).__init__()\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, l, g):\n",
        "        N, C, W, H = l.size()\n",
        "        c = self.op(l+g) # batch_sizex1xWxH\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "        if self.normalize_attn:\n",
        "            g = g.view(N,C,-1).sum(dim=2) # batch_sizexC\n",
        "        else:\n",
        "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "        return c.view(N,1,W,H), g\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-JYIOwBAF-S"
      },
      "source": [
        "class GridAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=False):\n",
        "        super(GridAttentionBlock, self).__init__()\n",
        "        self.up_factor = up_factor\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
        "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
        "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
        "    \n",
        "    def forward(self, l, g):\n",
        "        N, C, W, H = l.size()\n",
        "        l_ = self.W_l(l)\n",
        "        g_ = self.W_g(g)\n",
        "        if self.up_factor > 1:\n",
        "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
        "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
        "        # compute attn map\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        # re-weight the local feature\n",
        "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
        "        if self.normalize_attn:\n",
        "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
        "        else:\n",
        "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C)\n",
        "            \n",
        "        return c.view(N,1,W,H), output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6ObxVvAP55"
      },
      "source": [
        "def weights_init_xavierNormal(module):\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_normal_(m.weight, gain=np.sqrt(2))\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.constant_(m.bias, val=0.)\n",
        "        \n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=np.sqrt(2))\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, val=0.)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBJV74ILM3Tc"
      },
      "source": [
        "class AttnVGG(nn.Module):\n",
        "  def __init__(self, im_size, num_classes, attention=True, normalize_attn=True):\n",
        "    super(AttnVGG, self).__init__()\n",
        "    self.attention = attention\n",
        "\n",
        "    self.cv1 = ConvBlock(3,64,2)\n",
        "    self.cv2 = ConvBlock(64,128, 2)\n",
        "    self.cv3 = ConvBlock(128,256,3)\n",
        "    self.cv4 = ConvBlock(256,512,3)\n",
        "    self.cv5 = ConvBlock(512,512,3)\n",
        "    self.cv6 = ConvBlock(512,512,2, pool=True)\n",
        "    self.dense = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = int(im_size/32), padding = 0, bias = True)\n",
        "    #Attention = True\n",
        "    self.projector = ProjectorBlock(256, 512)\n",
        "    self.attn1 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)\n",
        "    self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)\n",
        "    self.attn3 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)      \n",
        "    #Final Classification Layer\n",
        "    self.classify = nn.Linear(in_features = 512 * 3, out_features = num_classes, bias = True)\n",
        "    #weight = U [-(1/sqrt(n)), 1/sqrt(n)]\n",
        "    weights_init_xavierNormal(self)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.cv1(x)\n",
        "    x = self.cv2(x)\n",
        "\n",
        "    l1 = self.cv3(x)\n",
        "    x = F.max_pool2d(l1, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    l2 = self.cv4(x)\n",
        "    x = F.max_pool2d(l2, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    l3 = self.cv5(x)\n",
        "    x = F.max_pool2d(l3, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    x = self.cv6(x)\n",
        "    g = self.dense(x)\n",
        "\n",
        "    #Attention part\n",
        "    c1, g1 = self.attn1(self.projector(l1), g)\n",
        "    c2, g2 = self.attn2(l2, g)\n",
        "    c3, g3 = self.attn3(l3, g)\n",
        "    g = torch.cat((g1,g2,g3), dim=1) # batch_sizexC\n",
        "    \n",
        "    # classification layer\n",
        "    x = self.classify(g) # batch_sizexnum_classes\n",
        "\n",
        "    return [x, c1, c2, c3]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0IFph49VjDM"
      },
      "source": [
        "%mkdir logs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuQubTcggaK"
      },
      "source": [
        "%matplotlib inline\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYnr1NuQFFJk"
      },
      "source": [
        "def train():\n",
        "  net = AttnVGG(im_size= 32, num_classes=100)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  epochs = 300\n",
        "  device = torch.device(\"cuda\")\n",
        "  device_ids = [0,]\n",
        "  model = nn.DataParallel(net, device_ids=device_ids).to(device)\n",
        "  criterion.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr= 0.1, momentum=0.9, weight_decay=5e-4)\n",
        "  lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n",
        "  scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "  \n",
        "  step = 0\n",
        "  running_avg_accuracy = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    images_disp = []\n",
        "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
        "    for aug in range(3):\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        if aug == 0 and i == 0:\n",
        "          images_disp.append(inputs[0:36, :,:,:])\n",
        "          # forward\n",
        "          pred, __, __, __ = model(inputs)\n",
        "          # backward\n",
        "          loss = criterion(pred, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # display results\n",
        "          if i % 10 == 0:\n",
        "              model.eval()\n",
        "              pred, __, __, __ = model(inputs)\n",
        "              predict = torch.argmax(pred, 1)\n",
        "              total = labels.size(0)\n",
        "              correct = torch.eq(predict, labels).sum().double().item()\n",
        "              accuracy = correct / total\n",
        "              running_avg_accuracy = 0.9*running_avg_accuracy + 0.1*accuracy\n",
        "              \n",
        "              print(\"[epoch %d][aug %d/%d][%d/%d] loss %.4f accuracy %.2f%% running avg accuracy %.2f%%\"\n",
        "                  % (epoch, aug, 2, i, len(trainloader)-1, loss.item(), (100*accuracy), (100*running_avg_accuracy)))\n",
        "          step += 1        \n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(\"logs\", 'net.pth'))\n",
        "\n",
        "    if epoch == 150:\n",
        "      torch.save(model.state_dict(), os.path.join(\"logs\", 'net%d.pth' % epoch))\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        # log scalars\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            images_test, labels_test = data\n",
        "            images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
        "            if i == 0: # archive images in order to save to logs\n",
        "                images_disp.append(inputs[0:36,:,:,:])\n",
        "            pred_test, __, __, __ = model(images_test)\n",
        "            predict = torch.argmax(pred_test, 1)\n",
        "            total += labels_test.size(0)\n",
        "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
        "        \n",
        "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
        "      \n",
        "          #I_train = utils.make_grid(images_disp[0], nrow=6, normalize=True, scale_each=True)\n",
        "          #show(I_train)\n",
        "          #if epoch == 0:\n",
        "                    #I_test = utils.make_grid(images_disp[1], nrow=6, normalize=True, scale_each=True)\n",
        "                    #show(I_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckkm2LtDFFMA",
        "outputId": "c568149c-47c1-463d-807e-00ff9eb54929"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0 learning rate 0.100000\n",
            "\n",
            "[epoch 0][aug 0/2][0/781] loss 4.6051 accuracy 6.25% running avg accuracy 0.62%\n",
            "\n",
            "[epoch 0] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 1 learning rate 0.100000\n",
            "\n",
            "[epoch 1][aug 0/2][0/781] loss 4.6045 accuracy 3.12% running avg accuracy 0.88%\n",
            "\n",
            "[epoch 1] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 2 learning rate 0.100000\n",
            "\n",
            "[epoch 2][aug 0/2][0/781] loss 4.6080 accuracy 0.00% running avg accuracy 0.79%\n",
            "\n",
            "[epoch 2] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 3 learning rate 0.100000\n",
            "\n",
            "[epoch 3][aug 0/2][0/781] loss 4.6051 accuracy 0.00% running avg accuracy 0.71%\n",
            "\n",
            "[epoch 3] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 4 learning rate 0.100000\n",
            "\n",
            "[epoch 4][aug 0/2][0/781] loss 4.6120 accuracy 0.00% running avg accuracy 0.64%\n",
            "\n",
            "[epoch 4] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 5 learning rate 0.100000\n",
            "\n",
            "[epoch 5][aug 0/2][0/781] loss 4.6020 accuracy 1.56% running avg accuracy 0.73%\n",
            "\n",
            "[epoch 5] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 6 learning rate 0.100000\n",
            "\n",
            "[epoch 6][aug 0/2][0/781] loss 4.5988 accuracy 1.56% running avg accuracy 0.81%\n",
            "\n",
            "[epoch 6] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 7 learning rate 0.100000\n",
            "\n",
            "[epoch 7][aug 0/2][0/781] loss 4.5964 accuracy 1.56% running avg accuracy 0.89%\n",
            "\n",
            "[epoch 7] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 8 learning rate 0.100000\n",
            "\n",
            "[epoch 8][aug 0/2][0/781] loss 4.6029 accuracy 0.00% running avg accuracy 0.80%\n",
            "\n",
            "[epoch 8] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 9 learning rate 0.100000\n",
            "\n",
            "[epoch 9][aug 0/2][0/781] loss 4.5909 accuracy 0.00% running avg accuracy 0.72%\n",
            "\n",
            "[epoch 9] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 10 learning rate 0.100000\n",
            "\n",
            "[epoch 10][aug 0/2][0/781] loss 4.5944 accuracy 0.00% running avg accuracy 0.65%\n",
            "\n",
            "[epoch 10] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 11 learning rate 0.100000\n",
            "\n",
            "[epoch 11][aug 0/2][0/781] loss 4.6013 accuracy 1.56% running avg accuracy 0.74%\n",
            "\n",
            "[epoch 11] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 12 learning rate 0.100000\n",
            "\n",
            "[epoch 12][aug 0/2][0/781] loss 4.5986 accuracy 0.00% running avg accuracy 0.67%\n",
            "\n",
            "[epoch 12] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 13 learning rate 0.100000\n",
            "\n",
            "[epoch 13][aug 0/2][0/781] loss 4.6070 accuracy 3.12% running avg accuracy 0.91%\n",
            "\n",
            "[epoch 13] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 14 learning rate 0.100000\n",
            "\n",
            "[epoch 14][aug 0/2][0/781] loss 4.5756 accuracy 1.56% running avg accuracy 0.98%\n",
            "\n",
            "[epoch 14] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 15 learning rate 0.100000\n",
            "\n",
            "[epoch 15][aug 0/2][0/781] loss 4.5893 accuracy 1.56% running avg accuracy 1.03%\n",
            "\n",
            "[epoch 15] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 16 learning rate 0.100000\n",
            "\n",
            "[epoch 16][aug 0/2][0/781] loss 4.5927 accuracy 0.00% running avg accuracy 0.93%\n",
            "\n",
            "[epoch 16] accuracy on test data: 1.00%\n",
            "\n",
            "\n",
            "epoch 17 learning rate 0.100000\n",
            "\n",
            "[epoch 17][aug 0/2][0/781] loss 4.5800 accuracy 1.56% running avg accuracy 0.99%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}