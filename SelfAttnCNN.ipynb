{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SelfAttnCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec522f4bc52d4f3a9f7c8193782d022f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a0267e45b89044149303abb42f775736",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_db3ddb55755c4b9ebaa9d58d015bcfa4",
              "IPY_MODEL_187eedf807e34bc9be24392ddb0dcfe5"
            ]
          }
        },
        "a0267e45b89044149303abb42f775736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db3ddb55755c4b9ebaa9d58d015bcfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b8377d1f7bb74d1f865cfbfeaa3ee71b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2786e82f95a402cbde7ead45b0aa51c"
          }
        },
        "187eedf807e34bc9be24392ddb0dcfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4947a3a51ec840f1bf7a4cf68cad0d20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:05&lt;00:00, 29137545.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8ed9b611bfb430d8e7dc34aeb109046"
          }
        },
        "b8377d1f7bb74d1f865cfbfeaa3ee71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2786e82f95a402cbde7ead45b0aa51c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4947a3a51ec840f1bf7a4cf68cad0d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8ed9b611bfb430d8e7dc34aeb109046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVp4ygtM3Rr"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torchvision\n",
        "import torchvision.utils as utils\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EG4hac_FFN2"
      },
      "source": [
        "def _worker_init_fn_():\n",
        "    torch_seed = torch.initial_seed()\n",
        "    np_seed = torch_seed // 2**32-1\n",
        "    random.seed(torch_seed)\n",
        "    np.random.seed(np_seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNwqqh3fFFQ8"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfm7oFtq25vS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "ec522f4bc52d4f3a9f7c8193782d022f",
            "a0267e45b89044149303abb42f775736",
            "db3ddb55755c4b9ebaa9d58d015bcfa4",
            "187eedf807e34bc9be24392ddb0dcfe5",
            "b8377d1f7bb74d1f865cfbfeaa3ee71b",
            "b2786e82f95a402cbde7ead45b0aa51c",
            "4947a3a51ec840f1bf7a4cf68cad0d20",
            "a8ed9b611bfb430d8e7dc34aeb109046"
          ]
        },
        "outputId": "de9ba5b8-dd68-4c1b-d408-2dee6e56983a"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='CIFAR10_data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size= 64, shuffle=True, num_workers=2, worker_init_fn=_worker_init_fn_())\n",
        "testset = torchvision.datasets.CIFAR10(root='CIFAR10_data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10_data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec522f4bc52d4f3a9f7c8193782d022f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting CIFAR10_data/cifar-10-python.tar.gz to CIFAR10_data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1MrdPUi_43e"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        features = [in_features] + [out_features for i in range(num_conv)]\n",
        "        layers = []\n",
        "        for i in range(len(features)-1):\n",
        "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU())\n",
        "            if pool:\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        self.op = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I18g3Vfh_5Db"
      },
      "source": [
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, inputs):\n",
        "        return self.op(inputs)\n",
        "        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmMT3in_5FK"
      },
      "source": [
        "class LinearAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features, normalize_attn=True):\n",
        "        super(LinearAttentionBlock, self).__init__()\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, l, g):\n",
        "        N, C, W, H = l.size()\n",
        "        c = self.op(l+g) # batch_sizex1xWxH\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "        if self.normalize_attn:\n",
        "            g = g.view(N,C,-1).sum(dim=2) # batch_sizexC\n",
        "        else:\n",
        "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "        return c.view(N,1,W,H), g\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-JYIOwBAF-S"
      },
      "source": [
        "class GridAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=False):\n",
        "        super(GridAttentionBlock, self).__init__()\n",
        "        self.up_factor = up_factor\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
        "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
        "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
        "    \n",
        "    def forward(self, l, g):\n",
        "        N, C, W, H = l.size()\n",
        "        l_ = self.W_l(l)\n",
        "        g_ = self.W_g(g)\n",
        "        if self.up_factor > 1:\n",
        "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
        "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
        "        # compute attn map\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        # re-weight the local feature\n",
        "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
        "        if self.normalize_attn:\n",
        "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
        "        else:\n",
        "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C)\n",
        "            \n",
        "        return c.view(N,1,W,H), output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6ObxVvAP55"
      },
      "source": [
        "def weights_init_xavierNormal(module):\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_normal_(m.weight, gain=np.sqrt(2))\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.constant_(m.bias, val=0.)\n",
        "        \n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=np.sqrt(2))\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, val=0.)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBJV74ILM3Tc"
      },
      "source": [
        "class AttnVGG(nn.Module):\n",
        "  def __init__(self, im_size, num_classes, attention=True, normalize_attn=True):\n",
        "    super(AttnVGG, self).__init__()\n",
        "    self.attention = attention\n",
        "    self.memory = {}\n",
        "    self.cv1 = ConvBlock(3,64,2)\n",
        "    self.cv2 = ConvBlock(64,128, 2)\n",
        "    self.cv3 = ConvBlock(128,256,3)\n",
        "    self.cv4 = ConvBlock(256,512,3)\n",
        "    self.cv5 = ConvBlock(512,512,3)\n",
        "    self.cv6 = ConvBlock(512,512,2, pool=True)\n",
        "    self.dense = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = int(im_size/32), padding = 0, bias = True)\n",
        "    #Attention = True\n",
        "    self.projector = ProjectorBlock(256, 512)\n",
        "    self.attn1 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)\n",
        "    self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)\n",
        "    self.attn3 = LinearAttentionBlock(in_features=512, normalize_attn= normalize_attn)      \n",
        "    #Final Classification Layer\n",
        "    self.classify = nn.Linear(in_features = 512 * 3, out_features = num_classes, bias = True)\n",
        "    #weight = U [-(1/sqrt(n)), 1/sqrt(n)]\n",
        "    weights_init_xavierNormal(self)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.cv1(x)\n",
        "    x = self.cv2(x)\n",
        "    #self.memory[]\n",
        "    l1 = self.cv3(x)\n",
        "    x = F.max_pool2d(l1, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    l2 = self.cv4(x)\n",
        "    x = F.max_pool2d(l2, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    l3 = self.cv5(x)\n",
        "    x = F.max_pool2d(l3, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "    x = self.cv6(x)\n",
        "    g = self.dense(x)\n",
        "\n",
        "    #Attention part\n",
        "    c1, g1 = self.attn1(self.projector(l1), g)\n",
        "    c2, g2 = self.attn2(l2, g)\n",
        "    c3, g3 = self.attn3(l3, g)\n",
        "    g = torch.cat((g1,g2,g3), dim=1) # batch_sizexC\n",
        "    np = c1.detach().cpu().numpy()\n",
        "  \n",
        "    # classification layer\n",
        "    x = self.classify(g) # batch_sizexnum_classes\n",
        "    return [x, c1, c2, c3]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0IFph49VjDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7f1429-83e9-496c-8862-49588f6ef0eb"
      },
      "source": [
        "%mkdir logs"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜logsâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuQubTcggaK"
      },
      "source": [
        "%matplotlib inline\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYnr1NuQFFJk"
      },
      "source": [
        "def train():\n",
        "  net = AttnVGG(im_size= 32, num_classes=100)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  epochs = 300\n",
        "  device = torch.device(\"cuda\")\n",
        "  device_ids = [0,]\n",
        "  model = nn.DataParallel(net, device_ids=device_ids).to(device)\n",
        "  criterion.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr= 0.1, momentum=0.9, weight_decay=5e-4)\n",
        "  lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n",
        "  scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "  \n",
        "  step = 0\n",
        "  running_avg_accuracy = 0\n",
        "  aug =0\n",
        "  for epoch in range(epochs):\n",
        "    images_disp = []\n",
        "    print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      model.train()\n",
        "      model.zero_grad()\n",
        "      optimizer.zero_grad()\n",
        "      inputs, labels = data\n",
        "      \n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "    \n",
        "      images_disp.append(inputs[0:36, :,:,:])\n",
        "      # forward\n",
        "      pred, __, __, __= model(inputs)\n",
        "      # backward\n",
        "      loss = criterion(pred, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i % 100 == 0:\n",
        "          model.eval()\n",
        "          pred, __, __, __ = model(inputs)\n",
        "          predict = torch.argmax(pred, 1)\n",
        "          total = labels.size(0)\n",
        "          correct = torch.eq(predict, labels).sum().double().item()\n",
        "          accuracy = correct / total\n",
        "          running_avg_accuracy = 0.9*running_avg_accuracy + 0.1*accuracy\n",
        "          \n",
        "          print(\"[epoch %d][aug %d/%d][%d/%d] loss %.4f accuracy %.2f%% running avg accuracy %.2f%%\"\n",
        "              % (epoch, aug, 2, i, len(trainloader)-1, loss.item(), (100*accuracy), (100*running_avg_accuracy)))\n",
        "      step += 1       \n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(\"logs\", 'net.pth'))\n",
        "\n",
        "    if epoch == 150:\n",
        "      torch.save(model.state_dict(), os.path.join(\"logs\", 'net%d.pth' % epoch))\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        # log scalars\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            images_test, labels_test = data\n",
        "            images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
        "            if i == 0: # archive images in order to save to logs\n",
        "                images_disp.append(inputs[0:36,:,:,:])\n",
        "            pred_test, __, __, __ = model(images_test)\n",
        "            predict = torch.argmax(pred_test, 1)\n",
        "            total += labels_test.size(0)\n",
        "            correct += torch.eq(predict, labels_test).sum().double().item()\n",
        "        \n",
        "        print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n",
        "      \n",
        "          #I_train = utils.make_grid(images_disp[0], nrow=6, normalize=True, scale_each=True)\n",
        "          #show(I_train)\n",
        "          #if epoch == 0:\n",
        "                    #I_test = utils.make_grid(images_disp[1], nrow=6, normalize=True, scale_each=True)\n",
        "                    #show(I_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckkm2LtDFFMA",
        "outputId": "c1cc06de-9413-4993-fe09-7ff7b7aebc92"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0 learning rate 0.100000\n",
            "\n",
            "[epoch 0][aug 0/2][0/781] loss 4.6052 accuracy 14.06% running avg accuracy 1.41%\n",
            "[epoch 0][aug 0/2][100/781] loss 2.0601 accuracy 18.75% running avg accuracy 3.14%\n",
            "[epoch 0][aug 0/2][200/781] loss 1.8635 accuracy 42.19% running avg accuracy 7.05%\n",
            "[epoch 0][aug 0/2][300/781] loss 1.9348 accuracy 37.50% running avg accuracy 10.09%\n",
            "[epoch 0][aug 0/2][400/781] loss 1.9965 accuracy 28.12% running avg accuracy 11.89%\n",
            "[epoch 0][aug 0/2][500/781] loss 1.8461 accuracy 28.12% running avg accuracy 13.52%\n",
            "[epoch 0][aug 0/2][600/781] loss 1.8785 accuracy 15.62% running avg accuracy 13.73%\n",
            "[epoch 0][aug 0/2][700/781] loss 1.3641 accuracy 53.12% running avg accuracy 17.67%\n",
            "\n",
            "[epoch 0] accuracy on test data: 40.76%\n",
            "\n",
            "\n",
            "epoch 1 learning rate 0.100000\n",
            "\n",
            "[epoch 1][aug 0/2][0/781] loss 1.3852 accuracy 51.56% running avg accuracy 21.06%\n",
            "[epoch 1][aug 0/2][100/781] loss 1.5296 accuracy 29.69% running avg accuracy 21.92%\n",
            "[epoch 1][aug 0/2][200/781] loss 1.5576 accuracy 39.06% running avg accuracy 23.63%\n",
            "[epoch 1][aug 0/2][300/781] loss 1.0551 accuracy 54.69% running avg accuracy 26.74%\n",
            "[epoch 1][aug 0/2][400/781] loss 1.1035 accuracy 43.75% running avg accuracy 28.44%\n",
            "[epoch 1][aug 0/2][500/781] loss 1.2213 accuracy 53.12% running avg accuracy 30.91%\n",
            "[epoch 1][aug 0/2][600/781] loss 1.1005 accuracy 56.25% running avg accuracy 33.44%\n",
            "[epoch 1][aug 0/2][700/781] loss 1.3837 accuracy 53.12% running avg accuracy 35.41%\n",
            "\n",
            "[epoch 1] accuracy on test data: 50.56%\n",
            "\n",
            "\n",
            "epoch 2 learning rate 0.100000\n",
            "\n",
            "[epoch 2][aug 0/2][0/781] loss 1.2579 accuracy 48.44% running avg accuracy 36.71%\n",
            "[epoch 2][aug 0/2][100/781] loss 1.2210 accuracy 43.75% running avg accuracy 37.42%\n",
            "[epoch 2][aug 0/2][200/781] loss 1.2533 accuracy 56.25% running avg accuracy 39.30%\n",
            "[epoch 2][aug 0/2][300/781] loss 1.0326 accuracy 48.44% running avg accuracy 40.21%\n",
            "[epoch 2][aug 0/2][400/781] loss 0.8660 accuracy 65.62% running avg accuracy 42.76%\n",
            "[epoch 2][aug 0/2][500/781] loss 0.9963 accuracy 54.69% running avg accuracy 43.95%\n",
            "[epoch 2][aug 0/2][600/781] loss 1.1422 accuracy 57.81% running avg accuracy 45.34%\n",
            "[epoch 2][aug 0/2][700/781] loss 0.8408 accuracy 62.50% running avg accuracy 47.05%\n",
            "\n",
            "[epoch 2] accuracy on test data: 58.52%\n",
            "\n",
            "\n",
            "epoch 3 learning rate 0.100000\n",
            "\n",
            "[epoch 3][aug 0/2][0/781] loss 1.2627 accuracy 59.38% running avg accuracy 48.28%\n",
            "[epoch 3][aug 0/2][100/781] loss 1.1094 accuracy 35.94% running avg accuracy 47.05%\n",
            "[epoch 3][aug 0/2][200/781] loss 0.8860 accuracy 62.50% running avg accuracy 48.59%\n",
            "[epoch 3][aug 0/2][300/781] loss 0.9903 accuracy 62.50% running avg accuracy 49.98%\n",
            "[epoch 3][aug 0/2][400/781] loss 0.9206 accuracy 56.25% running avg accuracy 50.61%\n",
            "[epoch 3][aug 0/2][500/781] loss 0.8513 accuracy 51.56% running avg accuracy 50.71%\n",
            "[epoch 3][aug 0/2][600/781] loss 1.0490 accuracy 64.06% running avg accuracy 52.04%\n",
            "[epoch 3][aug 0/2][700/781] loss 0.7396 accuracy 65.62% running avg accuracy 53.40%\n",
            "\n",
            "[epoch 3] accuracy on test data: 57.11%\n",
            "\n",
            "\n",
            "epoch 4 learning rate 0.100000\n",
            "\n",
            "[epoch 4][aug 0/2][0/781] loss 0.8864 accuracy 60.94% running avg accuracy 54.15%\n",
            "[epoch 4][aug 0/2][100/781] loss 0.9679 accuracy 67.19% running avg accuracy 55.46%\n",
            "[epoch 4][aug 0/2][200/781] loss 0.7718 accuracy 64.06% running avg accuracy 56.32%\n",
            "[epoch 4][aug 0/2][300/781] loss 0.9913 accuracy 67.19% running avg accuracy 57.40%\n",
            "[epoch 4][aug 0/2][400/781] loss 0.8906 accuracy 53.12% running avg accuracy 56.98%\n",
            "[epoch 4][aug 0/2][500/781] loss 0.8853 accuracy 70.31% running avg accuracy 58.31%\n",
            "[epoch 4][aug 0/2][600/781] loss 0.9829 accuracy 59.38% running avg accuracy 58.42%\n",
            "[epoch 4][aug 0/2][700/781] loss 0.7724 accuracy 71.88% running avg accuracy 59.76%\n",
            "\n",
            "[epoch 4] accuracy on test data: 68.15%\n",
            "\n",
            "\n",
            "epoch 5 learning rate 0.100000\n",
            "\n",
            "[epoch 5][aug 0/2][0/781] loss 0.8887 accuracy 68.75% running avg accuracy 60.66%\n",
            "[epoch 5][aug 0/2][100/781] loss 0.6761 accuracy 76.56% running avg accuracy 62.25%\n",
            "[epoch 5][aug 0/2][200/781] loss 0.9299 accuracy 73.44% running avg accuracy 63.37%\n",
            "[epoch 5][aug 0/2][300/781] loss 0.5702 accuracy 71.88% running avg accuracy 64.22%\n",
            "[epoch 5][aug 0/2][400/781] loss 0.7477 accuracy 64.06% running avg accuracy 64.20%\n",
            "[epoch 5][aug 0/2][500/781] loss 0.7398 accuracy 76.56% running avg accuracy 65.44%\n",
            "[epoch 5][aug 0/2][600/781] loss 0.6649 accuracy 68.75% running avg accuracy 65.77%\n",
            "[epoch 5][aug 0/2][700/781] loss 0.8946 accuracy 68.75% running avg accuracy 66.07%\n",
            "\n",
            "[epoch 5] accuracy on test data: 57.42%\n",
            "\n",
            "\n",
            "epoch 6 learning rate 0.100000\n",
            "\n",
            "[epoch 6][aug 0/2][0/781] loss 0.7133 accuracy 56.25% running avg accuracy 65.09%\n",
            "[epoch 6][aug 0/2][100/781] loss 0.7363 accuracy 79.69% running avg accuracy 66.55%\n",
            "[epoch 6][aug 0/2][200/781] loss 0.8419 accuracy 64.06% running avg accuracy 66.30%\n",
            "[epoch 6][aug 0/2][300/781] loss 0.7063 accuracy 73.44% running avg accuracy 67.01%\n",
            "[epoch 6][aug 0/2][400/781] loss 0.8589 accuracy 70.31% running avg accuracy 67.34%\n",
            "[epoch 6][aug 0/2][500/781] loss 0.7770 accuracy 75.00% running avg accuracy 68.11%\n",
            "[epoch 6][aug 0/2][600/781] loss 0.5950 accuracy 73.44% running avg accuracy 68.64%\n",
            "[epoch 6][aug 0/2][700/781] loss 0.6503 accuracy 75.00% running avg accuracy 69.28%\n",
            "\n",
            "[epoch 6] accuracy on test data: 70.37%\n",
            "\n",
            "\n",
            "epoch 7 learning rate 0.100000\n",
            "\n",
            "[epoch 7][aug 0/2][0/781] loss 0.5922 accuracy 75.00% running avg accuracy 69.85%\n",
            "[epoch 7][aug 0/2][100/781] loss 0.6323 accuracy 81.25% running avg accuracy 70.99%\n",
            "[epoch 7][aug 0/2][200/781] loss 0.7469 accuracy 70.31% running avg accuracy 70.92%\n",
            "[epoch 7][aug 0/2][300/781] loss 0.3898 accuracy 73.44% running avg accuracy 71.17%\n",
            "[epoch 7][aug 0/2][400/781] loss 0.4896 accuracy 76.56% running avg accuracy 71.71%\n",
            "[epoch 7][aug 0/2][500/781] loss 0.6724 accuracy 70.31% running avg accuracy 71.57%\n",
            "[epoch 7][aug 0/2][600/781] loss 0.5300 accuracy 82.81% running avg accuracy 72.70%\n",
            "[epoch 7][aug 0/2][700/781] loss 0.5501 accuracy 81.25% running avg accuracy 73.55%\n",
            "\n",
            "[epoch 7] accuracy on test data: 71.29%\n",
            "\n",
            "\n",
            "epoch 8 learning rate 0.100000\n",
            "\n",
            "[epoch 8][aug 0/2][0/781] loss 0.6338 accuracy 65.62% running avg accuracy 72.76%\n",
            "[epoch 8][aug 0/2][100/781] loss 0.5968 accuracy 78.12% running avg accuracy 73.30%\n",
            "[epoch 8][aug 0/2][200/781] loss 0.8301 accuracy 70.31% running avg accuracy 73.00%\n",
            "[epoch 8][aug 0/2][300/781] loss 0.4435 accuracy 81.25% running avg accuracy 73.82%\n",
            "[epoch 8][aug 0/2][400/781] loss 0.7360 accuracy 73.44% running avg accuracy 73.78%\n",
            "[epoch 8][aug 0/2][500/781] loss 0.5466 accuracy 68.75% running avg accuracy 73.28%\n",
            "[epoch 8][aug 0/2][600/781] loss 0.5233 accuracy 82.81% running avg accuracy 74.23%\n",
            "[epoch 8][aug 0/2][700/781] loss 0.4147 accuracy 82.81% running avg accuracy 75.09%\n",
            "\n",
            "[epoch 8] accuracy on test data: 75.74%\n",
            "\n",
            "\n",
            "epoch 9 learning rate 0.100000\n",
            "\n",
            "[epoch 9][aug 0/2][0/781] loss 0.8310 accuracy 73.44% running avg accuracy 74.93%\n",
            "[epoch 9][aug 0/2][100/781] loss 0.7007 accuracy 60.94% running avg accuracy 73.53%\n",
            "[epoch 9][aug 0/2][200/781] loss 0.5643 accuracy 82.81% running avg accuracy 74.46%\n",
            "[epoch 9][aug 0/2][300/781] loss 0.6702 accuracy 75.00% running avg accuracy 74.51%\n",
            "[epoch 9][aug 0/2][400/781] loss 0.7125 accuracy 68.75% running avg accuracy 73.93%\n",
            "[epoch 9][aug 0/2][500/781] loss 0.8652 accuracy 75.00% running avg accuracy 74.04%\n",
            "[epoch 9][aug 0/2][600/781] loss 0.7513 accuracy 73.44% running avg accuracy 73.98%\n",
            "[epoch 9][aug 0/2][700/781] loss 0.6172 accuracy 65.62% running avg accuracy 73.14%\n",
            "\n",
            "[epoch 9] accuracy on test data: 69.42%\n",
            "\n",
            "\n",
            "epoch 10 learning rate 0.100000\n",
            "\n",
            "[epoch 10][aug 0/2][0/781] loss 0.4974 accuracy 71.88% running avg accuracy 73.02%\n",
            "[epoch 10][aug 0/2][100/781] loss 0.4985 accuracy 81.25% running avg accuracy 73.84%\n",
            "[epoch 10][aug 0/2][200/781] loss 0.3717 accuracy 82.81% running avg accuracy 74.74%\n",
            "[epoch 10][aug 0/2][300/781] loss 0.4583 accuracy 78.12% running avg accuracy 75.08%\n",
            "[epoch 10][aug 0/2][400/781] loss 0.7252 accuracy 76.56% running avg accuracy 75.23%\n",
            "[epoch 10][aug 0/2][500/781] loss 0.3934 accuracy 79.69% running avg accuracy 75.67%\n",
            "[epoch 10][aug 0/2][600/781] loss 0.4163 accuracy 76.56% running avg accuracy 75.76%\n",
            "[epoch 10][aug 0/2][700/781] loss 0.5900 accuracy 81.25% running avg accuracy 76.31%\n",
            "\n",
            "[epoch 10] accuracy on test data: 76.95%\n",
            "\n",
            "\n",
            "epoch 11 learning rate 0.100000\n",
            "\n",
            "[epoch 11][aug 0/2][0/781] loss 0.4261 accuracy 85.94% running avg accuracy 77.27%\n",
            "[epoch 11][aug 0/2][100/781] loss 0.7180 accuracy 71.88% running avg accuracy 76.73%\n",
            "[epoch 11][aug 0/2][200/781] loss 0.6487 accuracy 81.25% running avg accuracy 77.18%\n",
            "[epoch 11][aug 0/2][300/781] loss 0.7431 accuracy 84.38% running avg accuracy 77.90%\n",
            "[epoch 11][aug 0/2][400/781] loss 0.5000 accuracy 57.81% running avg accuracy 75.89%\n",
            "[epoch 11][aug 0/2][500/781] loss 0.5301 accuracy 87.50% running avg accuracy 77.05%\n",
            "[epoch 11][aug 0/2][600/781] loss 0.7138 accuracy 81.25% running avg accuracy 77.47%\n",
            "[epoch 11][aug 0/2][700/781] loss 0.3922 accuracy 75.00% running avg accuracy 77.23%\n",
            "\n",
            "[epoch 11] accuracy on test data: 77.75%\n",
            "\n",
            "\n",
            "epoch 12 learning rate 0.100000\n",
            "\n",
            "[epoch 12][aug 0/2][0/781] loss 0.3365 accuracy 93.75% running avg accuracy 78.88%\n",
            "[epoch 12][aug 0/2][100/781] loss 0.6132 accuracy 73.44% running avg accuracy 78.34%\n",
            "[epoch 12][aug 0/2][200/781] loss 0.6603 accuracy 76.56% running avg accuracy 78.16%\n",
            "[epoch 12][aug 0/2][300/781] loss 0.5792 accuracy 62.50% running avg accuracy 76.59%\n",
            "[epoch 12][aug 0/2][400/781] loss 0.5223 accuracy 75.00% running avg accuracy 76.43%\n",
            "[epoch 12][aug 0/2][500/781] loss 0.6041 accuracy 73.44% running avg accuracy 76.13%\n",
            "[epoch 12][aug 0/2][600/781] loss 0.6051 accuracy 70.31% running avg accuracy 75.55%\n",
            "[epoch 12][aug 0/2][700/781] loss 0.4757 accuracy 82.81% running avg accuracy 76.28%\n",
            "\n",
            "[epoch 12] accuracy on test data: 74.78%\n",
            "\n",
            "\n",
            "epoch 13 learning rate 0.100000\n",
            "\n",
            "[epoch 13][aug 0/2][0/781] loss 0.4094 accuracy 81.25% running avg accuracy 76.77%\n",
            "[epoch 13][aug 0/2][100/781] loss 0.6402 accuracy 78.12% running avg accuracy 76.91%\n",
            "[epoch 13][aug 0/2][200/781] loss 0.3633 accuracy 90.62% running avg accuracy 78.28%\n",
            "[epoch 13][aug 0/2][300/781] loss 0.6020 accuracy 73.44% running avg accuracy 77.80%\n",
            "[epoch 13][aug 0/2][400/781] loss 1.0147 accuracy 64.06% running avg accuracy 76.42%\n",
            "[epoch 13][aug 0/2][500/781] loss 0.7469 accuracy 64.06% running avg accuracy 75.19%\n",
            "[epoch 13][aug 0/2][600/781] loss 0.3924 accuracy 93.75% running avg accuracy 77.04%\n",
            "[epoch 13][aug 0/2][700/781] loss 0.5971 accuracy 79.69% running avg accuracy 77.31%\n",
            "\n",
            "[epoch 13] accuracy on test data: 78.87%\n",
            "\n",
            "\n",
            "epoch 14 learning rate 0.100000\n",
            "\n",
            "[epoch 14][aug 0/2][0/781] loss 0.5538 accuracy 81.25% running avg accuracy 77.70%\n",
            "[epoch 14][aug 0/2][100/781] loss 0.7549 accuracy 82.81% running avg accuracy 78.21%\n",
            "[epoch 14][aug 0/2][200/781] loss 0.5367 accuracy 73.44% running avg accuracy 77.74%\n",
            "[epoch 14][aug 0/2][300/781] loss 0.4479 accuracy 85.94% running avg accuracy 78.56%\n",
            "[epoch 14][aug 0/2][400/781] loss 0.5543 accuracy 79.69% running avg accuracy 78.67%\n",
            "[epoch 14][aug 0/2][500/781] loss 0.4288 accuracy 90.62% running avg accuracy 79.86%\n",
            "[epoch 14][aug 0/2][600/781] loss 0.5994 accuracy 81.25% running avg accuracy 80.00%\n",
            "[epoch 14][aug 0/2][700/781] loss 0.5172 accuracy 81.25% running avg accuracy 80.13%\n",
            "\n",
            "[epoch 14] accuracy on test data: 77.65%\n",
            "\n",
            "\n",
            "epoch 15 learning rate 0.100000\n",
            "\n",
            "[epoch 15][aug 0/2][0/781] loss 0.3718 accuracy 85.94% running avg accuracy 80.71%\n",
            "[epoch 15][aug 0/2][100/781] loss 0.5316 accuracy 79.69% running avg accuracy 80.61%\n",
            "[epoch 15][aug 0/2][200/781] loss 0.7432 accuracy 79.69% running avg accuracy 80.51%\n",
            "[epoch 15][aug 0/2][300/781] loss 0.3514 accuracy 85.94% running avg accuracy 81.06%\n",
            "[epoch 15][aug 0/2][400/781] loss 0.4516 accuracy 82.81% running avg accuracy 81.23%\n",
            "[epoch 15][aug 0/2][500/781] loss 0.3815 accuracy 84.38% running avg accuracy 81.55%\n",
            "[epoch 15][aug 0/2][600/781] loss 0.4616 accuracy 67.19% running avg accuracy 80.11%\n",
            "[epoch 15][aug 0/2][700/781] loss 0.5075 accuracy 84.38% running avg accuracy 80.54%\n",
            "\n",
            "[epoch 15] accuracy on test data: 77.73%\n",
            "\n",
            "\n",
            "epoch 16 learning rate 0.100000\n",
            "\n",
            "[epoch 16][aug 0/2][0/781] loss 0.5258 accuracy 81.25% running avg accuracy 80.61%\n",
            "[epoch 16][aug 0/2][100/781] loss 0.5292 accuracy 84.38% running avg accuracy 80.99%\n",
            "[epoch 16][aug 0/2][200/781] loss 0.5352 accuracy 84.38% running avg accuracy 81.32%\n",
            "[epoch 16][aug 0/2][300/781] loss 0.6218 accuracy 84.38% running avg accuracy 81.63%\n",
            "[epoch 16][aug 0/2][400/781] loss 0.7215 accuracy 75.00% running avg accuracy 80.97%\n",
            "[epoch 16][aug 0/2][500/781] loss 0.6304 accuracy 82.81% running avg accuracy 81.15%\n",
            "[epoch 16][aug 0/2][600/781] loss 0.5888 accuracy 79.69% running avg accuracy 81.00%\n",
            "[epoch 16][aug 0/2][700/781] loss 0.6962 accuracy 73.44% running avg accuracy 80.25%\n",
            "\n",
            "[epoch 16] accuracy on test data: 73.95%\n",
            "\n",
            "\n",
            "epoch 17 learning rate 0.100000\n",
            "\n",
            "[epoch 17][aug 0/2][0/781] loss 0.7583 accuracy 70.31% running avg accuracy 79.25%\n",
            "[epoch 17][aug 0/2][100/781] loss 0.7645 accuracy 76.56% running avg accuracy 78.99%\n",
            "[epoch 17][aug 0/2][200/781] loss 0.8468 accuracy 75.00% running avg accuracy 78.59%\n",
            "[epoch 17][aug 0/2][300/781] loss 0.5252 accuracy 75.00% running avg accuracy 78.23%\n",
            "[epoch 17][aug 0/2][400/781] loss 0.5374 accuracy 73.44% running avg accuracy 77.75%\n",
            "[epoch 17][aug 0/2][500/781] loss 0.5619 accuracy 84.38% running avg accuracy 78.41%\n",
            "[epoch 17][aug 0/2][600/781] loss 0.4414 accuracy 84.38% running avg accuracy 79.01%\n",
            "[epoch 17][aug 0/2][700/781] loss 0.3621 accuracy 84.38% running avg accuracy 79.54%\n",
            "\n",
            "[epoch 17] accuracy on test data: 77.27%\n",
            "\n",
            "\n",
            "epoch 18 learning rate 0.100000\n",
            "\n",
            "[epoch 18][aug 0/2][0/781] loss 0.5713 accuracy 76.56% running avg accuracy 79.25%\n",
            "[epoch 18][aug 0/2][100/781] loss 0.4908 accuracy 84.38% running avg accuracy 79.76%\n",
            "[epoch 18][aug 0/2][200/781] loss 0.5115 accuracy 85.94% running avg accuracy 80.38%\n",
            "[epoch 18][aug 0/2][300/781] loss 0.6160 accuracy 67.19% running avg accuracy 79.06%\n",
            "[epoch 18][aug 0/2][400/781] loss 0.4976 accuracy 87.50% running avg accuracy 79.90%\n",
            "[epoch 18][aug 0/2][500/781] loss 0.6898 accuracy 67.19% running avg accuracy 78.63%\n",
            "[epoch 18][aug 0/2][600/781] loss 0.3619 accuracy 81.25% running avg accuracy 78.89%\n",
            "[epoch 18][aug 0/2][700/781] loss 0.5471 accuracy 79.69% running avg accuracy 78.97%\n",
            "\n",
            "[epoch 18] accuracy on test data: 75.43%\n",
            "\n",
            "\n",
            "epoch 19 learning rate 0.100000\n",
            "\n",
            "[epoch 19][aug 0/2][0/781] loss 0.6109 accuracy 81.25% running avg accuracy 79.20%\n",
            "[epoch 19][aug 0/2][100/781] loss 0.2141 accuracy 90.62% running avg accuracy 80.34%\n",
            "[epoch 19][aug 0/2][200/781] loss 0.2175 accuracy 89.06% running avg accuracy 81.21%\n",
            "[epoch 19][aug 0/2][300/781] loss 0.6714 accuracy 76.56% running avg accuracy 80.75%\n",
            "[epoch 19][aug 0/2][400/781] loss 0.5391 accuracy 71.88% running avg accuracy 79.86%\n",
            "[epoch 19][aug 0/2][500/781] loss 0.5358 accuracy 67.19% running avg accuracy 78.59%\n",
            "[epoch 19][aug 0/2][600/781] loss 0.9398 accuracy 65.62% running avg accuracy 77.30%\n",
            "[epoch 19][aug 0/2][700/781] loss 0.3618 accuracy 89.06% running avg accuracy 78.47%\n",
            "\n",
            "[epoch 19] accuracy on test data: 80.57%\n",
            "\n",
            "\n",
            "epoch 20 learning rate 0.100000\n",
            "\n",
            "[epoch 20][aug 0/2][0/781] loss 0.7027 accuracy 73.44% running avg accuracy 77.97%\n",
            "[epoch 20][aug 0/2][100/781] loss 0.4242 accuracy 64.06% running avg accuracy 76.58%\n",
            "[epoch 20][aug 0/2][200/781] loss 0.5493 accuracy 85.94% running avg accuracy 77.52%\n",
            "[epoch 20][aug 0/2][300/781] loss 0.3880 accuracy 82.81% running avg accuracy 78.05%\n",
            "[epoch 20][aug 0/2][400/781] loss 0.6769 accuracy 85.94% running avg accuracy 78.83%\n",
            "[epoch 20][aug 0/2][500/781] loss 0.4024 accuracy 73.44% running avg accuracy 78.29%\n",
            "[epoch 20][aug 0/2][600/781] loss 0.4962 accuracy 78.12% running avg accuracy 78.28%\n",
            "[epoch 20][aug 0/2][700/781] loss 0.2851 accuracy 89.06% running avg accuracy 79.36%\n",
            "\n",
            "[epoch 20] accuracy on test data: 74.11%\n",
            "\n",
            "\n",
            "epoch 21 learning rate 0.100000\n",
            "\n",
            "[epoch 21][aug 0/2][0/781] loss 0.5292 accuracy 76.56% running avg accuracy 79.08%\n",
            "[epoch 21][aug 0/2][100/781] loss 0.6858 accuracy 71.88% running avg accuracy 78.36%\n",
            "[epoch 21][aug 0/2][200/781] loss 0.4626 accuracy 82.81% running avg accuracy 78.80%\n",
            "[epoch 21][aug 0/2][300/781] loss 0.4313 accuracy 85.94% running avg accuracy 79.52%\n",
            "[epoch 21][aug 0/2][400/781] loss 0.4314 accuracy 85.94% running avg accuracy 80.16%\n",
            "[epoch 21][aug 0/2][500/781] loss 0.5747 accuracy 84.38% running avg accuracy 80.58%\n",
            "[epoch 21][aug 0/2][600/781] loss 0.5351 accuracy 85.94% running avg accuracy 81.12%\n",
            "[epoch 21][aug 0/2][700/781] loss 0.5800 accuracy 78.12% running avg accuracy 80.82%\n",
            "\n",
            "[epoch 21] accuracy on test data: 78.51%\n",
            "\n",
            "\n",
            "epoch 22 learning rate 0.100000\n",
            "\n",
            "[epoch 22][aug 0/2][0/781] loss 0.5376 accuracy 82.81% running avg accuracy 81.02%\n",
            "[epoch 22][aug 0/2][100/781] loss 0.6071 accuracy 81.25% running avg accuracy 81.04%\n",
            "[epoch 22][aug 0/2][200/781] loss 0.4114 accuracy 87.50% running avg accuracy 81.69%\n",
            "[epoch 22][aug 0/2][300/781] loss 0.6196 accuracy 70.31% running avg accuracy 80.55%\n",
            "[epoch 22][aug 0/2][400/781] loss 0.4746 accuracy 87.50% running avg accuracy 81.24%\n",
            "[epoch 22][aug 0/2][500/781] loss 0.5020 accuracy 90.62% running avg accuracy 82.18%\n",
            "[epoch 22][aug 0/2][600/781] loss 0.5689 accuracy 65.62% running avg accuracy 80.53%\n",
            "[epoch 22][aug 0/2][700/781] loss 0.4211 accuracy 85.94% running avg accuracy 81.07%\n",
            "\n",
            "[epoch 22] accuracy on test data: 72.56%\n",
            "\n",
            "\n",
            "epoch 23 learning rate 0.100000\n",
            "\n",
            "[epoch 23][aug 0/2][0/781] loss 0.3743 accuracy 76.56% running avg accuracy 80.62%\n",
            "[epoch 23][aug 0/2][100/781] loss 0.6441 accuracy 71.88% running avg accuracy 79.74%\n",
            "[epoch 23][aug 0/2][200/781] loss 0.5165 accuracy 79.69% running avg accuracy 79.74%\n",
            "[epoch 23][aug 0/2][300/781] loss 0.5557 accuracy 84.38% running avg accuracy 80.20%\n",
            "[epoch 23][aug 0/2][400/781] loss 0.4564 accuracy 78.12% running avg accuracy 79.99%\n",
            "[epoch 23][aug 0/2][500/781] loss 0.3652 accuracy 76.56% running avg accuracy 79.65%\n",
            "[epoch 23][aug 0/2][600/781] loss 0.3821 accuracy 75.00% running avg accuracy 79.19%\n",
            "[epoch 23][aug 0/2][700/781] loss 0.7003 accuracy 70.31% running avg accuracy 78.30%\n",
            "\n",
            "[epoch 23] accuracy on test data: 73.73%\n",
            "\n",
            "\n",
            "epoch 24 learning rate 0.100000\n",
            "\n",
            "[epoch 24][aug 0/2][0/781] loss 0.4780 accuracy 82.81% running avg accuracy 78.75%\n",
            "[epoch 24][aug 0/2][100/781] loss 0.4117 accuracy 84.38% running avg accuracy 79.31%\n",
            "[epoch 24][aug 0/2][200/781] loss 0.4960 accuracy 78.12% running avg accuracy 79.19%\n",
            "[epoch 24][aug 0/2][300/781] loss 0.5771 accuracy 84.38% running avg accuracy 79.71%\n",
            "[epoch 24][aug 0/2][400/781] loss 0.3989 accuracy 85.94% running avg accuracy 80.33%\n",
            "[epoch 24][aug 0/2][500/781] loss 0.6191 accuracy 79.69% running avg accuracy 80.27%\n",
            "[epoch 24][aug 0/2][600/781] loss 0.5594 accuracy 73.44% running avg accuracy 79.59%\n",
            "[epoch 24][aug 0/2][700/781] loss 0.5818 accuracy 81.25% running avg accuracy 79.75%\n",
            "\n",
            "[epoch 24] accuracy on test data: 79.45%\n",
            "\n",
            "\n",
            "epoch 25 learning rate 0.100000\n",
            "\n",
            "[epoch 25][aug 0/2][0/781] loss 0.7048 accuracy 75.00% running avg accuracy 79.28%\n",
            "[epoch 25][aug 0/2][100/781] loss 0.3521 accuracy 65.62% running avg accuracy 77.91%\n",
            "[epoch 25][aug 0/2][200/781] loss 0.6145 accuracy 85.94% running avg accuracy 78.71%\n",
            "[epoch 25][aug 0/2][300/781] loss 0.3946 accuracy 87.50% running avg accuracy 79.59%\n",
            "[epoch 25][aug 0/2][400/781] loss 0.5090 accuracy 70.31% running avg accuracy 78.67%\n",
            "[epoch 25][aug 0/2][500/781] loss 0.6765 accuracy 76.56% running avg accuracy 78.45%\n",
            "[epoch 25][aug 0/2][600/781] loss 0.6066 accuracy 65.62% running avg accuracy 77.17%\n",
            "[epoch 25][aug 0/2][700/781] loss 0.6279 accuracy 79.69% running avg accuracy 77.42%\n",
            "\n",
            "[epoch 25] accuracy on test data: 73.23%\n",
            "\n",
            "\n",
            "epoch 26 learning rate 0.100000\n",
            "\n",
            "[epoch 26][aug 0/2][0/781] loss 0.7081 accuracy 75.00% running avg accuracy 77.18%\n",
            "[epoch 26][aug 0/2][100/781] loss 0.4442 accuracy 84.38% running avg accuracy 77.90%\n",
            "[epoch 26][aug 0/2][200/781] loss 0.9161 accuracy 75.00% running avg accuracy 77.61%\n",
            "[epoch 26][aug 0/2][300/781] loss 0.3699 accuracy 90.62% running avg accuracy 78.91%\n",
            "[epoch 26][aug 0/2][400/781] loss 0.5279 accuracy 81.25% running avg accuracy 79.15%\n",
            "[epoch 26][aug 0/2][500/781] loss 0.5458 accuracy 81.25% running avg accuracy 79.36%\n",
            "[epoch 26][aug 0/2][600/781] loss 0.5029 accuracy 78.12% running avg accuracy 79.23%\n",
            "[epoch 26][aug 0/2][700/781] loss 0.3045 accuracy 92.19% running avg accuracy 80.53%\n",
            "\n",
            "[epoch 26] accuracy on test data: 72.31%\n",
            "\n",
            "\n",
            "epoch 27 learning rate 0.100000\n",
            "\n",
            "[epoch 27][aug 0/2][0/781] loss 0.5467 accuracy 73.44% running avg accuracy 79.82%\n",
            "[epoch 27][aug 0/2][100/781] loss 0.3763 accuracy 84.38% running avg accuracy 80.27%\n",
            "[epoch 27][aug 0/2][200/781] loss 0.3121 accuracy 78.12% running avg accuracy 80.06%\n",
            "[epoch 27][aug 0/2][300/781] loss 0.4129 accuracy 71.88% running avg accuracy 79.24%\n",
            "[epoch 27][aug 0/2][400/781] loss 0.4574 accuracy 81.25% running avg accuracy 79.44%\n",
            "[epoch 27][aug 0/2][500/781] loss 0.5216 accuracy 85.94% running avg accuracy 80.09%\n",
            "[epoch 27][aug 0/2][600/781] loss 0.3735 accuracy 79.69% running avg accuracy 80.05%\n",
            "[epoch 27][aug 0/2][700/781] loss 0.4738 accuracy 92.19% running avg accuracy 81.26%\n",
            "\n",
            "[epoch 27] accuracy on test data: 72.53%\n",
            "\n",
            "\n",
            "epoch 28 learning rate 0.100000\n",
            "\n",
            "[epoch 28][aug 0/2][0/781] loss 0.3861 accuracy 89.06% running avg accuracy 82.04%\n",
            "[epoch 28][aug 0/2][100/781] loss 0.5480 accuracy 89.06% running avg accuracy 82.75%\n",
            "[epoch 28][aug 0/2][200/781] loss 0.3143 accuracy 84.38% running avg accuracy 82.91%\n",
            "[epoch 28][aug 0/2][300/781] loss 0.4248 accuracy 84.38% running avg accuracy 83.06%\n",
            "[epoch 28][aug 0/2][400/781] loss 0.3681 accuracy 87.50% running avg accuracy 83.50%\n",
            "[epoch 28][aug 0/2][500/781] loss 0.5947 accuracy 85.94% running avg accuracy 83.74%\n",
            "[epoch 28][aug 0/2][600/781] loss 0.3381 accuracy 84.38% running avg accuracy 83.81%\n",
            "[epoch 28][aug 0/2][700/781] loss 0.4537 accuracy 82.81% running avg accuracy 83.71%\n",
            "\n",
            "[epoch 28] accuracy on test data: 74.40%\n",
            "\n",
            "\n",
            "epoch 29 learning rate 0.100000\n",
            "\n",
            "[epoch 29][aug 0/2][0/781] loss 0.5038 accuracy 79.69% running avg accuracy 83.31%\n",
            "[epoch 29][aug 0/2][100/781] loss 0.5811 accuracy 79.69% running avg accuracy 82.94%\n",
            "[epoch 29][aug 0/2][200/781] loss 0.3648 accuracy 82.81% running avg accuracy 82.93%\n",
            "[epoch 29][aug 0/2][300/781] loss 0.5027 accuracy 87.50% running avg accuracy 83.39%\n",
            "[epoch 29][aug 0/2][400/781] loss 0.3367 accuracy 84.38% running avg accuracy 83.49%\n",
            "[epoch 29][aug 0/2][500/781] loss 0.4363 accuracy 89.06% running avg accuracy 84.04%\n",
            "[epoch 29][aug 0/2][600/781] loss 0.6283 accuracy 82.81% running avg accuracy 83.92%\n",
            "[epoch 29][aug 0/2][700/781] loss 0.4403 accuracy 87.50% running avg accuracy 84.28%\n",
            "\n",
            "[epoch 29] accuracy on test data: 77.04%\n",
            "\n",
            "\n",
            "epoch 30 learning rate 0.100000\n",
            "\n",
            "[epoch 30][aug 0/2][0/781] loss 0.4858 accuracy 78.12% running avg accuracy 83.66%\n",
            "[epoch 30][aug 0/2][100/781] loss 0.4253 accuracy 75.00% running avg accuracy 82.80%\n",
            "[epoch 30][aug 0/2][200/781] loss 0.5865 accuracy 70.31% running avg accuracy 81.55%\n",
            "[epoch 30][aug 0/2][300/781] loss 0.4627 accuracy 82.81% running avg accuracy 81.67%\n",
            "[epoch 30][aug 0/2][400/781] loss 0.4173 accuracy 78.12% running avg accuracy 81.32%\n",
            "[epoch 30][aug 0/2][500/781] loss 0.5214 accuracy 84.38% running avg accuracy 81.63%\n",
            "[epoch 30][aug 0/2][600/781] loss 0.4782 accuracy 78.12% running avg accuracy 81.28%\n",
            "[epoch 30][aug 0/2][700/781] loss 0.4535 accuracy 87.50% running avg accuracy 81.90%\n",
            "\n",
            "[epoch 30] accuracy on test data: 75.89%\n",
            "\n",
            "\n",
            "epoch 31 learning rate 0.100000\n",
            "\n",
            "[epoch 31][aug 0/2][0/781] loss 0.4216 accuracy 84.38% running avg accuracy 82.15%\n",
            "[epoch 31][aug 0/2][100/781] loss 0.8291 accuracy 76.56% running avg accuracy 81.59%\n",
            "[epoch 31][aug 0/2][200/781] loss 0.4394 accuracy 82.81% running avg accuracy 81.71%\n",
            "[epoch 31][aug 0/2][300/781] loss 0.5410 accuracy 79.69% running avg accuracy 81.51%\n",
            "[epoch 31][aug 0/2][400/781] loss 0.5231 accuracy 71.88% running avg accuracy 80.54%\n",
            "[epoch 31][aug 0/2][500/781] loss 0.3714 accuracy 87.50% running avg accuracy 81.24%\n",
            "[epoch 31][aug 0/2][600/781] loss 0.5296 accuracy 79.69% running avg accuracy 81.08%\n",
            "[epoch 31][aug 0/2][700/781] loss 0.5598 accuracy 71.88% running avg accuracy 80.16%\n",
            "\n",
            "[epoch 31] accuracy on test data: 74.17%\n",
            "\n",
            "\n",
            "epoch 32 learning rate 0.100000\n",
            "\n",
            "[epoch 32][aug 0/2][0/781] loss 0.6761 accuracy 65.62% running avg accuracy 78.71%\n",
            "[epoch 32][aug 0/2][100/781] loss 0.6225 accuracy 84.38% running avg accuracy 79.28%\n",
            "[epoch 32][aug 0/2][200/781] loss 0.4626 accuracy 84.38% running avg accuracy 79.79%\n",
            "[epoch 32][aug 0/2][300/781] loss 0.4801 accuracy 92.19% running avg accuracy 81.03%\n",
            "[epoch 32][aug 0/2][400/781] loss 0.4997 accuracy 89.06% running avg accuracy 81.83%\n",
            "[epoch 32][aug 0/2][500/781] loss 0.5560 accuracy 81.25% running avg accuracy 81.77%\n",
            "[epoch 32][aug 0/2][600/781] loss 0.4954 accuracy 84.38% running avg accuracy 82.03%\n",
            "[epoch 32][aug 0/2][700/781] loss 0.4842 accuracy 87.50% running avg accuracy 82.58%\n",
            "\n",
            "[epoch 32] accuracy on test data: 81.09%\n",
            "\n",
            "\n",
            "epoch 33 learning rate 0.100000\n",
            "\n",
            "[epoch 33][aug 0/2][0/781] loss 0.5575 accuracy 90.62% running avg accuracy 83.38%\n",
            "[epoch 33][aug 0/2][100/781] loss 0.3971 accuracy 82.81% running avg accuracy 83.33%\n",
            "[epoch 33][aug 0/2][200/781] loss 0.4023 accuracy 75.00% running avg accuracy 82.49%\n",
            "[epoch 33][aug 0/2][300/781] loss 0.4789 accuracy 81.25% running avg accuracy 82.37%\n",
            "[epoch 33][aug 0/2][400/781] loss 0.5048 accuracy 73.44% running avg accuracy 81.48%\n",
            "[epoch 33][aug 0/2][500/781] loss 0.3756 accuracy 92.19% running avg accuracy 82.55%\n",
            "[epoch 33][aug 0/2][600/781] loss 0.7092 accuracy 75.00% running avg accuracy 81.79%\n",
            "[epoch 33][aug 0/2][700/781] loss 0.3775 accuracy 84.38% running avg accuracy 82.05%\n",
            "\n",
            "[epoch 33] accuracy on test data: 77.69%\n",
            "\n",
            "\n",
            "epoch 34 learning rate 0.100000\n",
            "\n",
            "[epoch 34][aug 0/2][0/781] loss 0.4928 accuracy 78.12% running avg accuracy 81.66%\n",
            "[epoch 34][aug 0/2][100/781] loss 0.5532 accuracy 84.38% running avg accuracy 81.93%\n",
            "[epoch 34][aug 0/2][200/781] loss 0.3309 accuracy 89.06% running avg accuracy 82.64%\n",
            "[epoch 34][aug 0/2][300/781] loss 0.5603 accuracy 79.69% running avg accuracy 82.35%\n",
            "[epoch 34][aug 0/2][400/781] loss 0.5838 accuracy 82.81% running avg accuracy 82.39%\n",
            "[epoch 34][aug 0/2][500/781] loss 0.4934 accuracy 50.00% running avg accuracy 79.15%\n",
            "[epoch 34][aug 0/2][600/781] loss 0.5175 accuracy 76.56% running avg accuracy 78.90%\n",
            "[epoch 34][aug 0/2][700/781] loss 0.4799 accuracy 73.44% running avg accuracy 78.35%\n",
            "\n",
            "[epoch 34] accuracy on test data: 81.87%\n",
            "\n",
            "\n",
            "epoch 35 learning rate 0.100000\n",
            "\n",
            "[epoch 35][aug 0/2][0/781] loss 0.4714 accuracy 89.06% running avg accuracy 79.42%\n",
            "[epoch 35][aug 0/2][100/781] loss 0.5847 accuracy 85.94% running avg accuracy 80.07%\n",
            "[epoch 35][aug 0/2][200/781] loss 0.3430 accuracy 87.50% running avg accuracy 80.82%\n",
            "[epoch 35][aug 0/2][300/781] loss 0.6664 accuracy 71.88% running avg accuracy 79.92%\n",
            "[epoch 35][aug 0/2][400/781] loss 0.5237 accuracy 84.38% running avg accuracy 80.37%\n",
            "[epoch 35][aug 0/2][500/781] loss 0.4757 accuracy 64.06% running avg accuracy 78.74%\n",
            "[epoch 35][aug 0/2][600/781] loss 0.4308 accuracy 70.31% running avg accuracy 77.89%\n",
            "[epoch 35][aug 0/2][700/781] loss 0.4619 accuracy 90.62% running avg accuracy 79.17%\n",
            "\n",
            "[epoch 35] accuracy on test data: 79.72%\n",
            "\n",
            "\n",
            "epoch 36 learning rate 0.100000\n",
            "\n",
            "[epoch 36][aug 0/2][0/781] loss 0.3492 accuracy 95.31% running avg accuracy 80.78%\n",
            "[epoch 36][aug 0/2][100/781] loss 0.4687 accuracy 90.62% running avg accuracy 81.77%\n",
            "[epoch 36][aug 0/2][200/781] loss 0.5134 accuracy 81.25% running avg accuracy 81.71%\n",
            "[epoch 36][aug 0/2][300/781] loss 0.5041 accuracy 81.25% running avg accuracy 81.67%\n",
            "[epoch 36][aug 0/2][400/781] loss 0.6034 accuracy 60.94% running avg accuracy 79.59%\n",
            "[epoch 36][aug 0/2][500/781] loss 0.4152 accuracy 87.50% running avg accuracy 80.39%\n",
            "[epoch 36][aug 0/2][600/781] loss 0.4423 accuracy 84.38% running avg accuracy 80.78%\n",
            "[epoch 36][aug 0/2][700/781] loss 0.4594 accuracy 87.50% running avg accuracy 81.46%\n",
            "\n",
            "[epoch 36] accuracy on test data: 78.38%\n",
            "\n",
            "\n",
            "epoch 37 learning rate 0.100000\n",
            "\n",
            "[epoch 37][aug 0/2][0/781] loss 0.5008 accuracy 90.62% running avg accuracy 82.37%\n",
            "[epoch 37][aug 0/2][100/781] loss 0.4544 accuracy 82.81% running avg accuracy 82.42%\n",
            "[epoch 37][aug 0/2][200/781] loss 0.3414 accuracy 81.25% running avg accuracy 82.30%\n",
            "[epoch 37][aug 0/2][300/781] loss 0.4804 accuracy 87.50% running avg accuracy 82.82%\n",
            "[epoch 37][aug 0/2][400/781] loss 0.5915 accuracy 75.00% running avg accuracy 82.04%\n",
            "[epoch 37][aug 0/2][500/781] loss 0.5246 accuracy 79.69% running avg accuracy 81.80%\n",
            "[epoch 37][aug 0/2][600/781] loss 0.6175 accuracy 79.69% running avg accuracy 81.59%\n",
            "[epoch 37][aug 0/2][700/781] loss 0.3853 accuracy 75.00% running avg accuracy 80.93%\n",
            "\n",
            "[epoch 37] accuracy on test data: 78.44%\n",
            "\n",
            "\n",
            "epoch 38 learning rate 0.100000\n",
            "\n",
            "[epoch 38][aug 0/2][0/781] loss 0.3131 accuracy 82.81% running avg accuracy 81.12%\n",
            "[epoch 38][aug 0/2][100/781] loss 0.4828 accuracy 82.81% running avg accuracy 81.29%\n",
            "[epoch 38][aug 0/2][200/781] loss 0.4268 accuracy 85.94% running avg accuracy 81.75%\n",
            "[epoch 38][aug 0/2][300/781] loss 0.5093 accuracy 81.25% running avg accuracy 81.70%\n",
            "[epoch 38][aug 0/2][400/781] loss 0.6082 accuracy 87.50% running avg accuracy 82.28%\n",
            "[epoch 38][aug 0/2][500/781] loss 0.6392 accuracy 81.25% running avg accuracy 82.18%\n",
            "[epoch 38][aug 0/2][600/781] loss 0.3158 accuracy 93.75% running avg accuracy 83.34%\n",
            "[epoch 38][aug 0/2][700/781] loss 0.3624 accuracy 87.50% running avg accuracy 83.75%\n",
            "\n",
            "[epoch 38] accuracy on test data: 75.92%\n",
            "\n",
            "\n",
            "epoch 39 learning rate 0.100000\n",
            "\n",
            "[epoch 39][aug 0/2][0/781] loss 0.3061 accuracy 82.81% running avg accuracy 83.66%\n",
            "[epoch 39][aug 0/2][100/781] loss 0.4089 accuracy 93.75% running avg accuracy 84.67%\n",
            "[epoch 39][aug 0/2][200/781] loss 0.2295 accuracy 92.19% running avg accuracy 85.42%\n",
            "[epoch 39][aug 0/2][300/781] loss 0.2440 accuracy 95.31% running avg accuracy 86.41%\n",
            "[epoch 39][aug 0/2][400/781] loss 0.3944 accuracy 85.94% running avg accuracy 86.36%\n",
            "[epoch 39][aug 0/2][500/781] loss 0.6780 accuracy 68.75% running avg accuracy 84.60%\n",
            "[epoch 39][aug 0/2][600/781] loss 0.5142 accuracy 81.25% running avg accuracy 84.27%\n",
            "[epoch 39][aug 0/2][700/781] loss 0.4796 accuracy 81.25% running avg accuracy 83.96%\n",
            "\n",
            "[epoch 39] accuracy on test data: 71.94%\n",
            "\n",
            "\n",
            "epoch 40 learning rate 0.100000\n",
            "\n",
            "[epoch 40][aug 0/2][0/781] loss 0.3005 accuracy 75.00% running avg accuracy 83.07%\n",
            "[epoch 40][aug 0/2][100/781] loss 0.4638 accuracy 84.38% running avg accuracy 83.20%\n",
            "[epoch 40][aug 0/2][200/781] loss 0.3907 accuracy 84.38% running avg accuracy 83.32%\n",
            "[epoch 40][aug 0/2][300/781] loss 0.5200 accuracy 79.69% running avg accuracy 82.95%\n",
            "[epoch 40][aug 0/2][400/781] loss 0.5902 accuracy 76.56% running avg accuracy 82.31%\n",
            "[epoch 40][aug 0/2][500/781] loss 0.3477 accuracy 89.06% running avg accuracy 82.99%\n",
            "[epoch 40][aug 0/2][600/781] loss 0.4522 accuracy 84.38% running avg accuracy 83.13%\n",
            "[epoch 40][aug 0/2][700/781] loss 0.3467 accuracy 96.88% running avg accuracy 84.50%\n",
            "\n",
            "[epoch 40] accuracy on test data: 76.78%\n",
            "\n",
            "\n",
            "epoch 41 learning rate 0.100000\n",
            "\n",
            "[epoch 41][aug 0/2][0/781] loss 0.5617 accuracy 75.00% running avg accuracy 83.55%\n",
            "[epoch 41][aug 0/2][100/781] loss 0.5913 accuracy 82.81% running avg accuracy 83.48%\n",
            "[epoch 41][aug 0/2][200/781] loss 0.5528 accuracy 84.38% running avg accuracy 83.57%\n",
            "[epoch 41][aug 0/2][300/781] loss 0.4342 accuracy 73.44% running avg accuracy 82.55%\n",
            "[epoch 41][aug 0/2][400/781] loss 0.4450 accuracy 92.19% running avg accuracy 83.52%\n",
            "[epoch 41][aug 0/2][500/781] loss 0.2777 accuracy 93.75% running avg accuracy 84.54%\n",
            "[epoch 41][aug 0/2][600/781] loss 0.5507 accuracy 76.56% running avg accuracy 83.74%\n",
            "[epoch 41][aug 0/2][700/781] loss 0.4199 accuracy 89.06% running avg accuracy 84.28%\n",
            "\n",
            "[epoch 41] accuracy on test data: 75.61%\n",
            "\n",
            "\n",
            "epoch 42 learning rate 0.100000\n",
            "\n",
            "[epoch 42][aug 0/2][0/781] loss 0.5402 accuracy 82.81% running avg accuracy 84.13%\n",
            "[epoch 42][aug 0/2][100/781] loss 0.4633 accuracy 84.38% running avg accuracy 84.15%\n",
            "[epoch 42][aug 0/2][200/781] loss 0.4126 accuracy 87.50% running avg accuracy 84.49%\n",
            "[epoch 42][aug 0/2][300/781] loss 0.3285 accuracy 85.94% running avg accuracy 84.63%\n",
            "[epoch 42][aug 0/2][400/781] loss 0.4493 accuracy 82.81% running avg accuracy 84.45%\n",
            "[epoch 42][aug 0/2][500/781] loss 0.2659 accuracy 90.62% running avg accuracy 85.07%\n",
            "[epoch 42][aug 0/2][600/781] loss 0.5764 accuracy 81.25% running avg accuracy 84.69%\n",
            "[epoch 42][aug 0/2][700/781] loss 0.5852 accuracy 75.00% running avg accuracy 83.72%\n",
            "\n",
            "[epoch 42] accuracy on test data: 74.78%\n",
            "\n",
            "\n",
            "epoch 43 learning rate 0.100000\n",
            "\n",
            "[epoch 43][aug 0/2][0/781] loss 0.7275 accuracy 67.19% running avg accuracy 82.06%\n",
            "[epoch 43][aug 0/2][100/781] loss 0.4406 accuracy 90.62% running avg accuracy 82.92%\n",
            "[epoch 43][aug 0/2][200/781] loss 0.3303 accuracy 76.56% running avg accuracy 82.29%\n",
            "[epoch 43][aug 0/2][300/781] loss 0.4317 accuracy 87.50% running avg accuracy 82.81%\n",
            "[epoch 43][aug 0/2][400/781] loss 0.3796 accuracy 81.25% running avg accuracy 82.65%\n",
            "[epoch 43][aug 0/2][500/781] loss 0.4920 accuracy 76.56% running avg accuracy 82.04%\n",
            "[epoch 43][aug 0/2][600/781] loss 0.3982 accuracy 76.56% running avg accuracy 81.49%\n",
            "[epoch 43][aug 0/2][700/781] loss 0.6439 accuracy 75.00% running avg accuracy 80.84%\n",
            "\n",
            "[epoch 43] accuracy on test data: 72.47%\n",
            "\n",
            "\n",
            "epoch 44 learning rate 0.100000\n",
            "\n",
            "[epoch 44][aug 0/2][0/781] loss 0.4464 accuracy 78.12% running avg accuracy 80.57%\n",
            "[epoch 44][aug 0/2][100/781] loss 0.2757 accuracy 85.94% running avg accuracy 81.11%\n",
            "[epoch 44][aug 0/2][200/781] loss 0.4944 accuracy 70.31% running avg accuracy 80.03%\n",
            "[epoch 44][aug 0/2][300/781] loss 0.4556 accuracy 79.69% running avg accuracy 80.00%\n",
            "[epoch 44][aug 0/2][400/781] loss 0.5962 accuracy 70.31% running avg accuracy 79.03%\n",
            "[epoch 44][aug 0/2][500/781] loss 0.4412 accuracy 87.50% running avg accuracy 79.87%\n",
            "[epoch 44][aug 0/2][600/781] loss 0.4294 accuracy 90.62% running avg accuracy 80.95%\n",
            "[epoch 44][aug 0/2][700/781] loss 0.4474 accuracy 87.50% running avg accuracy 81.60%\n",
            "\n",
            "[epoch 44] accuracy on test data: 76.56%\n",
            "\n",
            "\n",
            "epoch 45 learning rate 0.100000\n",
            "\n",
            "[epoch 45][aug 0/2][0/781] loss 0.4520 accuracy 79.69% running avg accuracy 81.41%\n",
            "[epoch 45][aug 0/2][100/781] loss 0.4809 accuracy 79.69% running avg accuracy 81.24%\n",
            "[epoch 45][aug 0/2][200/781] loss 0.2194 accuracy 89.06% running avg accuracy 82.02%\n",
            "[epoch 45][aug 0/2][300/781] loss 0.6143 accuracy 84.38% running avg accuracy 82.26%\n",
            "[epoch 45][aug 0/2][400/781] loss 0.6392 accuracy 79.69% running avg accuracy 82.00%\n",
            "[epoch 45][aug 0/2][500/781] loss 0.3916 accuracy 81.25% running avg accuracy 81.93%\n",
            "[epoch 45][aug 0/2][600/781] loss 0.7081 accuracy 82.81% running avg accuracy 82.01%\n",
            "[epoch 45][aug 0/2][700/781] loss 0.3367 accuracy 93.75% running avg accuracy 83.19%\n",
            "\n",
            "[epoch 45] accuracy on test data: 74.19%\n",
            "\n",
            "\n",
            "epoch 46 learning rate 0.100000\n",
            "\n",
            "[epoch 46][aug 0/2][0/781] loss 0.5264 accuracy 82.81% running avg accuracy 83.15%\n",
            "[epoch 46][aug 0/2][100/781] loss 0.3008 accuracy 82.81% running avg accuracy 83.12%\n",
            "[epoch 46][aug 0/2][200/781] loss 0.3873 accuracy 92.19% running avg accuracy 84.02%\n",
            "[epoch 46][aug 0/2][300/781] loss 0.3403 accuracy 78.12% running avg accuracy 83.43%\n",
            "[epoch 46][aug 0/2][400/781] loss 0.4379 accuracy 84.38% running avg accuracy 83.53%\n",
            "[epoch 46][aug 0/2][500/781] loss 0.5120 accuracy 84.38% running avg accuracy 83.61%\n",
            "[epoch 46][aug 0/2][600/781] loss 0.7518 accuracy 76.56% running avg accuracy 82.91%\n",
            "[epoch 46][aug 0/2][700/781] loss 0.3685 accuracy 78.12% running avg accuracy 82.43%\n",
            "\n",
            "[epoch 46] accuracy on test data: 79.56%\n",
            "\n",
            "\n",
            "epoch 47 learning rate 0.100000\n",
            "\n",
            "[epoch 47][aug 0/2][0/781] loss 0.4087 accuracy 85.94% running avg accuracy 82.78%\n",
            "[epoch 47][aug 0/2][100/781] loss 0.4665 accuracy 87.50% running avg accuracy 83.25%\n",
            "[epoch 47][aug 0/2][200/781] loss 0.4276 accuracy 89.06% running avg accuracy 83.83%\n",
            "[epoch 47][aug 0/2][300/781] loss 0.4274 accuracy 85.94% running avg accuracy 84.04%\n",
            "[epoch 47][aug 0/2][400/781] loss 0.5472 accuracy 81.25% running avg accuracy 83.76%\n",
            "[epoch 47][aug 0/2][500/781] loss 0.5044 accuracy 71.88% running avg accuracy 82.58%\n",
            "[epoch 47][aug 0/2][600/781] loss 0.4482 accuracy 78.12% running avg accuracy 82.13%\n",
            "[epoch 47][aug 0/2][700/781] loss 0.2602 accuracy 82.81% running avg accuracy 82.20%\n",
            "\n",
            "[epoch 47] accuracy on test data: 73.23%\n",
            "\n",
            "\n",
            "epoch 48 learning rate 0.100000\n",
            "\n",
            "[epoch 48][aug 0/2][0/781] loss 0.2842 accuracy 76.56% running avg accuracy 81.63%\n",
            "[epoch 48][aug 0/2][100/781] loss 0.3302 accuracy 75.00% running avg accuracy 80.97%\n",
            "[epoch 48][aug 0/2][200/781] loss 0.6732 accuracy 82.81% running avg accuracy 81.16%\n",
            "[epoch 48][aug 0/2][300/781] loss 0.5349 accuracy 76.56% running avg accuracy 80.70%\n",
            "[epoch 48][aug 0/2][400/781] loss 0.5305 accuracy 82.81% running avg accuracy 80.91%\n",
            "[epoch 48][aug 0/2][500/781] loss 0.3273 accuracy 87.50% running avg accuracy 81.57%\n",
            "[epoch 48][aug 0/2][600/781] loss 0.4091 accuracy 73.44% running avg accuracy 80.75%\n",
            "[epoch 48][aug 0/2][700/781] loss 0.5473 accuracy 68.75% running avg accuracy 79.55%\n",
            "\n",
            "[epoch 48] accuracy on test data: 69.55%\n",
            "\n",
            "\n",
            "epoch 49 learning rate 0.100000\n",
            "\n",
            "[epoch 49][aug 0/2][0/781] loss 0.5050 accuracy 60.94% running avg accuracy 77.69%\n",
            "[epoch 49][aug 0/2][100/781] loss 0.4221 accuracy 82.81% running avg accuracy 78.20%\n",
            "[epoch 49][aug 0/2][200/781] loss 0.8992 accuracy 82.81% running avg accuracy 78.66%\n",
            "[epoch 49][aug 0/2][300/781] loss 0.3698 accuracy 75.00% running avg accuracy 78.30%\n",
            "[epoch 49][aug 0/2][400/781] loss 0.4829 accuracy 75.00% running avg accuracy 77.97%\n",
            "[epoch 49][aug 0/2][500/781] loss 0.3227 accuracy 90.62% running avg accuracy 79.23%\n",
            "[epoch 49][aug 0/2][600/781] loss 0.3700 accuracy 92.19% running avg accuracy 80.53%\n",
            "[epoch 49][aug 0/2][700/781] loss 0.6809 accuracy 73.44% running avg accuracy 79.82%\n",
            "\n",
            "[epoch 49] accuracy on test data: 73.07%\n",
            "\n",
            "\n",
            "epoch 50 learning rate 0.100000\n",
            "\n",
            "[epoch 50][aug 0/2][0/781] loss 0.7337 accuracy 78.12% running avg accuracy 79.65%\n",
            "[epoch 50][aug 0/2][100/781] loss 0.4564 accuracy 82.81% running avg accuracy 79.97%\n",
            "[epoch 50][aug 0/2][200/781] loss 0.4578 accuracy 79.69% running avg accuracy 79.94%\n",
            "[epoch 50][aug 0/2][300/781] loss 0.4030 accuracy 84.38% running avg accuracy 80.38%\n",
            "[epoch 50][aug 0/2][400/781] loss 0.4470 accuracy 81.25% running avg accuracy 80.47%\n",
            "[epoch 50][aug 0/2][500/781] loss 0.3852 accuracy 84.38% running avg accuracy 80.86%\n",
            "[epoch 50][aug 0/2][600/781] loss 0.7043 accuracy 79.69% running avg accuracy 80.74%\n",
            "[epoch 50][aug 0/2][700/781] loss 0.4559 accuracy 89.06% running avg accuracy 81.57%\n",
            "\n",
            "[epoch 50] accuracy on test data: 75.57%\n",
            "\n",
            "\n",
            "epoch 51 learning rate 0.100000\n",
            "\n",
            "[epoch 51][aug 0/2][0/781] loss 0.3619 accuracy 75.00% running avg accuracy 80.92%\n",
            "[epoch 51][aug 0/2][100/781] loss 0.3972 accuracy 81.25% running avg accuracy 80.95%\n",
            "[epoch 51][aug 0/2][200/781] loss 0.4595 accuracy 78.12% running avg accuracy 80.67%\n",
            "[epoch 51][aug 0/2][300/781] loss 0.4462 accuracy 90.62% running avg accuracy 81.66%\n",
            "[epoch 51][aug 0/2][400/781] loss 0.4821 accuracy 82.81% running avg accuracy 81.78%\n",
            "[epoch 51][aug 0/2][500/781] loss 0.4380 accuracy 84.38% running avg accuracy 82.04%\n",
            "[epoch 51][aug 0/2][600/781] loss 0.5689 accuracy 84.38% running avg accuracy 82.27%\n",
            "[epoch 51][aug 0/2][700/781] loss 0.5093 accuracy 85.94% running avg accuracy 82.64%\n",
            "\n",
            "[epoch 51] accuracy on test data: 75.04%\n",
            "\n",
            "\n",
            "epoch 52 learning rate 0.100000\n",
            "\n",
            "[epoch 52][aug 0/2][0/781] loss 0.4428 accuracy 81.25% running avg accuracy 82.50%\n",
            "[epoch 52][aug 0/2][100/781] loss 0.3366 accuracy 81.25% running avg accuracy 82.37%\n",
            "[epoch 52][aug 0/2][200/781] loss 0.4247 accuracy 81.25% running avg accuracy 82.26%\n",
            "[epoch 52][aug 0/2][300/781] loss 0.6168 accuracy 75.00% running avg accuracy 81.54%\n",
            "[epoch 52][aug 0/2][400/781] loss 0.2991 accuracy 89.06% running avg accuracy 82.29%\n",
            "[epoch 52][aug 0/2][500/781] loss 0.3690 accuracy 54.69% running avg accuracy 79.53%\n",
            "[epoch 52][aug 0/2][600/781] loss 0.4583 accuracy 85.94% running avg accuracy 80.17%\n",
            "[epoch 52][aug 0/2][700/781] loss 0.3616 accuracy 85.94% running avg accuracy 80.75%\n",
            "\n",
            "[epoch 52] accuracy on test data: 77.51%\n",
            "\n",
            "\n",
            "epoch 53 learning rate 0.100000\n",
            "\n",
            "[epoch 53][aug 0/2][0/781] loss 0.2669 accuracy 82.81% running avg accuracy 80.95%\n",
            "[epoch 53][aug 0/2][100/781] loss 0.4828 accuracy 82.81% running avg accuracy 81.14%\n",
            "[epoch 53][aug 0/2][200/781] loss 0.4458 accuracy 82.81% running avg accuracy 81.31%\n",
            "[epoch 53][aug 0/2][300/781] loss 0.3880 accuracy 81.25% running avg accuracy 81.30%\n",
            "[epoch 53][aug 0/2][400/781] loss 0.3128 accuracy 84.38% running avg accuracy 81.61%\n",
            "[epoch 53][aug 0/2][500/781] loss 0.5090 accuracy 78.12% running avg accuracy 81.26%\n",
            "[epoch 53][aug 0/2][600/781] loss 0.6145 accuracy 78.12% running avg accuracy 80.95%\n",
            "[epoch 53][aug 0/2][700/781] loss 0.2562 accuracy 95.31% running avg accuracy 82.38%\n",
            "\n",
            "[epoch 53] accuracy on test data: 71.93%\n",
            "\n",
            "\n",
            "epoch 54 learning rate 0.100000\n",
            "\n",
            "[epoch 54][aug 0/2][0/781] loss 0.3842 accuracy 92.19% running avg accuracy 83.36%\n",
            "[epoch 54][aug 0/2][100/781] loss 0.5235 accuracy 82.81% running avg accuracy 83.31%\n",
            "[epoch 54][aug 0/2][200/781] loss 0.3477 accuracy 71.88% running avg accuracy 82.16%\n",
            "[epoch 54][aug 0/2][300/781] loss 0.4351 accuracy 87.50% running avg accuracy 82.70%\n",
            "[epoch 54][aug 0/2][400/781] loss 0.3378 accuracy 87.50% running avg accuracy 83.18%\n",
            "[epoch 54][aug 0/2][500/781] loss 0.4901 accuracy 90.62% running avg accuracy 83.92%\n",
            "[epoch 54][aug 0/2][600/781] loss 0.2827 accuracy 90.62% running avg accuracy 84.59%\n",
            "[epoch 54][aug 0/2][700/781] loss 0.5077 accuracy 75.00% running avg accuracy 83.63%\n",
            "\n",
            "[epoch 54] accuracy on test data: 70.82%\n",
            "\n",
            "\n",
            "epoch 55 learning rate 0.100000\n",
            "\n",
            "[epoch 55][aug 0/2][0/781] loss 0.3951 accuracy 67.19% running avg accuracy 81.99%\n",
            "[epoch 55][aug 0/2][100/781] loss 0.5622 accuracy 84.38% running avg accuracy 82.23%\n",
            "[epoch 55][aug 0/2][200/781] loss 0.6398 accuracy 73.44% running avg accuracy 81.35%\n",
            "[epoch 55][aug 0/2][300/781] loss 0.4231 accuracy 90.62% running avg accuracy 82.28%\n",
            "[epoch 55][aug 0/2][400/781] loss 0.3690 accuracy 79.69% running avg accuracy 82.02%\n",
            "[epoch 55][aug 0/2][500/781] loss 0.4378 accuracy 82.81% running avg accuracy 82.10%\n",
            "[epoch 55][aug 0/2][600/781] loss 0.3183 accuracy 93.75% running avg accuracy 83.26%\n",
            "[epoch 55][aug 0/2][700/781] loss 0.4871 accuracy 68.75% running avg accuracy 81.81%\n",
            "\n",
            "[epoch 55] accuracy on test data: 83.01%\n",
            "\n",
            "\n",
            "epoch 56 learning rate 0.100000\n",
            "\n",
            "[epoch 56][aug 0/2][0/781] loss 0.3982 accuracy 82.81% running avg accuracy 81.91%\n",
            "[epoch 56][aug 0/2][100/781] loss 0.3836 accuracy 81.25% running avg accuracy 81.85%\n",
            "[epoch 56][aug 0/2][200/781] loss 0.3654 accuracy 90.62% running avg accuracy 82.72%\n",
            "[epoch 56][aug 0/2][300/781] loss 0.3583 accuracy 87.50% running avg accuracy 83.20%\n",
            "[epoch 56][aug 0/2][400/781] loss 0.4262 accuracy 89.06% running avg accuracy 83.79%\n",
            "[epoch 56][aug 0/2][500/781] loss 0.6839 accuracy 78.12% running avg accuracy 83.22%\n",
            "[epoch 56][aug 0/2][600/781] loss 0.4711 accuracy 82.81% running avg accuracy 83.18%\n",
            "[epoch 56][aug 0/2][700/781] loss 0.4200 accuracy 89.06% running avg accuracy 83.77%\n",
            "\n",
            "[epoch 56] accuracy on test data: 80.11%\n",
            "\n",
            "\n",
            "epoch 57 learning rate 0.100000\n",
            "\n",
            "[epoch 57][aug 0/2][0/781] loss 0.2609 accuracy 90.62% running avg accuracy 84.45%\n",
            "[epoch 57][aug 0/2][100/781] loss 0.4242 accuracy 75.00% running avg accuracy 83.51%\n",
            "[epoch 57][aug 0/2][200/781] loss 0.3973 accuracy 89.06% running avg accuracy 84.06%\n",
            "[epoch 57][aug 0/2][300/781] loss 0.3010 accuracy 79.69% running avg accuracy 83.63%\n",
            "[epoch 57][aug 0/2][400/781] loss 0.4145 accuracy 73.44% running avg accuracy 82.61%\n",
            "[epoch 57][aug 0/2][500/781] loss 0.3326 accuracy 87.50% running avg accuracy 83.10%\n",
            "[epoch 57][aug 0/2][600/781] loss 0.4600 accuracy 87.50% running avg accuracy 83.54%\n",
            "[epoch 57][aug 0/2][700/781] loss 0.3707 accuracy 78.12% running avg accuracy 83.00%\n",
            "\n",
            "[epoch 57] accuracy on test data: 59.82%\n",
            "\n",
            "\n",
            "epoch 58 learning rate 0.100000\n",
            "\n",
            "[epoch 58][aug 0/2][0/781] loss 0.5645 accuracy 60.94% running avg accuracy 80.79%\n",
            "[epoch 58][aug 0/2][100/781] loss 0.3404 accuracy 84.38% running avg accuracy 81.15%\n",
            "[epoch 58][aug 0/2][200/781] loss 0.5367 accuracy 84.38% running avg accuracy 81.47%\n",
            "[epoch 58][aug 0/2][300/781] loss 0.6545 accuracy 79.69% running avg accuracy 81.29%\n",
            "[epoch 58][aug 0/2][400/781] loss 0.5480 accuracy 82.81% running avg accuracy 81.44%\n",
            "[epoch 58][aug 0/2][500/781] loss 0.6764 accuracy 68.75% running avg accuracy 80.18%\n",
            "[epoch 58][aug 0/2][600/781] loss 0.4316 accuracy 84.38% running avg accuracy 80.60%\n",
            "[epoch 58][aug 0/2][700/781] loss 0.5779 accuracy 70.31% running avg accuracy 79.57%\n",
            "\n",
            "[epoch 58] accuracy on test data: 80.30%\n",
            "\n",
            "\n",
            "epoch 59 learning rate 0.100000\n",
            "\n",
            "[epoch 59][aug 0/2][0/781] loss 0.3969 accuracy 92.19% running avg accuracy 80.83%\n",
            "[epoch 59][aug 0/2][100/781] loss 0.4153 accuracy 71.88% running avg accuracy 79.93%\n",
            "[epoch 59][aug 0/2][200/781] loss 0.4862 accuracy 87.50% running avg accuracy 80.69%\n",
            "[epoch 59][aug 0/2][300/781] loss 0.4765 accuracy 85.94% running avg accuracy 81.21%\n",
            "[epoch 59][aug 0/2][400/781] loss 0.3328 accuracy 82.81% running avg accuracy 81.37%\n",
            "[epoch 59][aug 0/2][500/781] loss 0.4674 accuracy 81.25% running avg accuracy 81.36%\n",
            "[epoch 59][aug 0/2][600/781] loss 0.4224 accuracy 84.38% running avg accuracy 81.66%\n",
            "[epoch 59][aug 0/2][700/781] loss 0.4271 accuracy 70.31% running avg accuracy 80.53%\n",
            "\n",
            "[epoch 59] accuracy on test data: 78.88%\n",
            "\n",
            "\n",
            "epoch 60 learning rate 0.100000\n",
            "\n",
            "[epoch 60][aug 0/2][0/781] loss 0.3748 accuracy 79.69% running avg accuracy 80.44%\n",
            "[epoch 60][aug 0/2][100/781] loss 0.5326 accuracy 84.38% running avg accuracy 80.84%\n",
            "[epoch 60][aug 0/2][200/781] loss 0.4884 accuracy 84.38% running avg accuracy 81.19%\n",
            "[epoch 60][aug 0/2][300/781] loss 0.4100 accuracy 81.25% running avg accuracy 81.20%\n",
            "[epoch 60][aug 0/2][400/781] loss 0.5959 accuracy 76.56% running avg accuracy 80.73%\n",
            "[epoch 60][aug 0/2][500/781] loss 0.4036 accuracy 85.94% running avg accuracy 81.25%\n",
            "[epoch 60][aug 0/2][600/781] loss 0.6537 accuracy 73.44% running avg accuracy 80.47%\n",
            "[epoch 60][aug 0/2][700/781] loss 0.4812 accuracy 79.69% running avg accuracy 80.39%\n",
            "\n",
            "[epoch 60] accuracy on test data: 80.21%\n",
            "\n",
            "\n",
            "epoch 61 learning rate 0.100000\n",
            "\n",
            "[epoch 61][aug 0/2][0/781] loss 0.4843 accuracy 85.94% running avg accuracy 80.95%\n",
            "[epoch 61][aug 0/2][100/781] loss 0.5503 accuracy 87.50% running avg accuracy 81.60%\n",
            "[epoch 61][aug 0/2][200/781] loss 0.3994 accuracy 75.00% running avg accuracy 80.94%\n",
            "[epoch 61][aug 0/2][300/781] loss 0.4068 accuracy 87.50% running avg accuracy 81.60%\n",
            "[epoch 61][aug 0/2][400/781] loss 0.4523 accuracy 87.50% running avg accuracy 82.19%\n",
            "[epoch 61][aug 0/2][500/781] loss 0.4632 accuracy 89.06% running avg accuracy 82.88%\n",
            "[epoch 61][aug 0/2][600/781] loss 0.5209 accuracy 87.50% running avg accuracy 83.34%\n",
            "[epoch 61][aug 0/2][700/781] loss 0.5659 accuracy 78.12% running avg accuracy 82.82%\n",
            "\n",
            "[epoch 61] accuracy on test data: 77.88%\n",
            "\n",
            "\n",
            "epoch 62 learning rate 0.100000\n",
            "\n",
            "[epoch 62][aug 0/2][0/781] loss 0.4399 accuracy 85.94% running avg accuracy 83.13%\n",
            "[epoch 62][aug 0/2][100/781] loss 0.2805 accuracy 85.94% running avg accuracy 83.41%\n",
            "[epoch 62][aug 0/2][200/781] loss 0.3809 accuracy 76.56% running avg accuracy 82.73%\n",
            "[epoch 62][aug 0/2][300/781] loss 0.7257 accuracy 79.69% running avg accuracy 82.42%\n",
            "[epoch 62][aug 0/2][400/781] loss 0.3618 accuracy 87.50% running avg accuracy 82.93%\n",
            "[epoch 62][aug 0/2][500/781] loss 0.2668 accuracy 87.50% running avg accuracy 83.39%\n",
            "[epoch 62][aug 0/2][600/781] loss 0.6215 accuracy 76.56% running avg accuracy 82.70%\n",
            "[epoch 62][aug 0/2][700/781] loss 0.5921 accuracy 87.50% running avg accuracy 83.18%\n",
            "\n",
            "[epoch 62] accuracy on test data: 79.14%\n",
            "\n",
            "\n",
            "epoch 63 learning rate 0.100000\n",
            "\n",
            "[epoch 63][aug 0/2][0/781] loss 0.3987 accuracy 89.06% running avg accuracy 83.77%\n",
            "[epoch 63][aug 0/2][100/781] loss 0.3929 accuracy 90.62% running avg accuracy 84.46%\n",
            "[epoch 63][aug 0/2][200/781] loss 0.2793 accuracy 85.94% running avg accuracy 84.60%\n",
            "[epoch 63][aug 0/2][300/781] loss 0.5067 accuracy 85.94% running avg accuracy 84.74%\n",
            "[epoch 63][aug 0/2][400/781] loss 0.5812 accuracy 76.56% running avg accuracy 83.92%\n",
            "[epoch 63][aug 0/2][500/781] loss 0.4071 accuracy 92.19% running avg accuracy 84.75%\n",
            "[epoch 63][aug 0/2][600/781] loss 0.5576 accuracy 82.81% running avg accuracy 84.55%\n",
            "[epoch 63][aug 0/2][700/781] loss 0.3283 accuracy 67.19% running avg accuracy 82.82%\n",
            "\n",
            "[epoch 63] accuracy on test data: 66.42%\n",
            "\n",
            "\n",
            "epoch 64 learning rate 0.100000\n",
            "\n",
            "[epoch 64][aug 0/2][0/781] loss 0.4268 accuracy 68.75% running avg accuracy 81.41%\n",
            "[epoch 64][aug 0/2][100/781] loss 0.5017 accuracy 76.56% running avg accuracy 80.93%\n",
            "[epoch 64][aug 0/2][200/781] loss 0.2661 accuracy 85.94% running avg accuracy 81.43%\n",
            "[epoch 64][aug 0/2][300/781] loss 0.4234 accuracy 78.12% running avg accuracy 81.10%\n",
            "[epoch 64][aug 0/2][400/781] loss 0.5207 accuracy 78.12% running avg accuracy 80.80%\n",
            "[epoch 64][aug 0/2][500/781] loss 0.5124 accuracy 75.00% running avg accuracy 80.22%\n",
            "[epoch 64][aug 0/2][600/781] loss 0.2626 accuracy 92.19% running avg accuracy 81.42%\n",
            "[epoch 64][aug 0/2][700/781] loss 0.4136 accuracy 89.06% running avg accuracy 82.18%\n",
            "\n",
            "[epoch 64] accuracy on test data: 67.86%\n",
            "\n",
            "\n",
            "epoch 65 learning rate 0.100000\n",
            "\n",
            "[epoch 65][aug 0/2][0/781] loss 0.2618 accuracy 73.44% running avg accuracy 81.31%\n",
            "[epoch 65][aug 0/2][100/781] loss 0.5717 accuracy 76.56% running avg accuracy 80.83%\n",
            "[epoch 65][aug 0/2][200/781] loss 0.6112 accuracy 62.50% running avg accuracy 79.00%\n",
            "[epoch 65][aug 0/2][300/781] loss 0.8211 accuracy 76.56% running avg accuracy 78.76%\n",
            "[epoch 65][aug 0/2][400/781] loss 0.3019 accuracy 84.38% running avg accuracy 79.32%\n",
            "[epoch 65][aug 0/2][500/781] loss 0.4272 accuracy 57.81% running avg accuracy 77.17%\n",
            "[epoch 65][aug 0/2][600/781] loss 0.3722 accuracy 87.50% running avg accuracy 78.20%\n",
            "[epoch 65][aug 0/2][700/781] loss 0.3781 accuracy 90.62% running avg accuracy 79.44%\n",
            "\n",
            "[epoch 65] accuracy on test data: 81.71%\n",
            "\n",
            "\n",
            "epoch 66 learning rate 0.100000\n",
            "\n",
            "[epoch 66][aug 0/2][0/781] loss 0.4490 accuracy 85.94% running avg accuracy 80.09%\n",
            "[epoch 66][aug 0/2][100/781] loss 0.4500 accuracy 73.44% running avg accuracy 79.43%\n",
            "[epoch 66][aug 0/2][200/781] loss 0.2749 accuracy 79.69% running avg accuracy 79.45%\n",
            "[epoch 66][aug 0/2][300/781] loss 0.3753 accuracy 78.12% running avg accuracy 79.32%\n",
            "[epoch 66][aug 0/2][400/781] loss 0.3683 accuracy 82.81% running avg accuracy 79.67%\n",
            "[epoch 66][aug 0/2][500/781] loss 0.4280 accuracy 78.12% running avg accuracy 79.51%\n",
            "[epoch 66][aug 0/2][600/781] loss 0.3886 accuracy 87.50% running avg accuracy 80.31%\n",
            "[epoch 66][aug 0/2][700/781] loss 0.4530 accuracy 92.19% running avg accuracy 81.50%\n",
            "\n",
            "[epoch 66] accuracy on test data: 69.85%\n",
            "\n",
            "\n",
            "epoch 67 learning rate 0.100000\n",
            "\n",
            "[epoch 67][aug 0/2][0/781] loss 0.5765 accuracy 76.56% running avg accuracy 81.01%\n",
            "[epoch 67][aug 0/2][100/781] loss 0.4173 accuracy 71.88% running avg accuracy 80.09%\n",
            "[epoch 67][aug 0/2][200/781] loss 0.5631 accuracy 84.38% running avg accuracy 80.52%\n",
            "[epoch 67][aug 0/2][300/781] loss 0.4243 accuracy 89.06% running avg accuracy 81.38%\n",
            "[epoch 67][aug 0/2][400/781] loss 0.4509 accuracy 76.56% running avg accuracy 80.89%\n",
            "[epoch 67][aug 0/2][500/781] loss 0.3029 accuracy 92.19% running avg accuracy 82.02%\n",
            "[epoch 67][aug 0/2][600/781] loss 0.4160 accuracy 81.25% running avg accuracy 81.95%\n",
            "[epoch 67][aug 0/2][700/781] loss 0.2299 accuracy 93.75% running avg accuracy 83.13%\n",
            "\n",
            "[epoch 67] accuracy on test data: 75.65%\n",
            "\n",
            "\n",
            "epoch 68 learning rate 0.100000\n",
            "\n",
            "[epoch 68][aug 0/2][0/781] loss 0.5153 accuracy 81.25% running avg accuracy 82.94%\n",
            "[epoch 68][aug 0/2][100/781] loss 0.3830 accuracy 79.69% running avg accuracy 82.61%\n",
            "[epoch 68][aug 0/2][200/781] loss 0.5349 accuracy 51.56% running avg accuracy 79.51%\n",
            "[epoch 68][aug 0/2][300/781] loss 0.5169 accuracy 82.81% running avg accuracy 79.84%\n",
            "[epoch 68][aug 0/2][400/781] loss 0.4806 accuracy 65.62% running avg accuracy 78.42%\n",
            "[epoch 68][aug 0/2][500/781] loss 0.4903 accuracy 71.88% running avg accuracy 77.76%\n",
            "[epoch 68][aug 0/2][600/781] loss 0.4079 accuracy 93.75% running avg accuracy 79.36%\n",
            "[epoch 68][aug 0/2][700/781] loss 0.3636 accuracy 67.19% running avg accuracy 78.14%\n",
            "\n",
            "[epoch 68] accuracy on test data: 75.01%\n",
            "\n",
            "\n",
            "epoch 69 learning rate 0.100000\n",
            "\n",
            "[epoch 69][aug 0/2][0/781] loss 0.3929 accuracy 79.69% running avg accuracy 78.30%\n",
            "[epoch 69][aug 0/2][100/781] loss 0.5198 accuracy 82.81% running avg accuracy 78.75%\n",
            "[epoch 69][aug 0/2][200/781] loss 0.6058 accuracy 70.31% running avg accuracy 77.91%\n",
            "[epoch 69][aug 0/2][300/781] loss 0.3216 accuracy 85.94% running avg accuracy 78.71%\n",
            "[epoch 69][aug 0/2][400/781] loss 0.8060 accuracy 81.25% running avg accuracy 78.96%\n",
            "[epoch 69][aug 0/2][500/781] loss 0.5446 accuracy 90.62% running avg accuracy 80.13%\n",
            "[epoch 69][aug 0/2][600/781] loss 0.6067 accuracy 79.69% running avg accuracy 80.09%\n",
            "[epoch 69][aug 0/2][700/781] loss 0.3717 accuracy 93.75% running avg accuracy 81.45%\n",
            "\n",
            "[epoch 69] accuracy on test data: 78.23%\n",
            "\n",
            "\n",
            "epoch 70 learning rate 0.100000\n",
            "\n",
            "[epoch 70][aug 0/2][0/781] loss 0.3156 accuracy 85.94% running avg accuracy 81.90%\n",
            "[epoch 70][aug 0/2][100/781] loss 0.5718 accuracy 89.06% running avg accuracy 82.62%\n",
            "[epoch 70][aug 0/2][200/781] loss 0.2728 accuracy 90.62% running avg accuracy 83.42%\n",
            "[epoch 70][aug 0/2][300/781] loss 0.3444 accuracy 82.81% running avg accuracy 83.36%\n",
            "[epoch 70][aug 0/2][400/781] loss 0.4399 accuracy 79.69% running avg accuracy 82.99%\n",
            "[epoch 70][aug 0/2][500/781] loss 0.3837 accuracy 92.19% running avg accuracy 83.91%\n",
            "[epoch 70][aug 0/2][600/781] loss 0.6036 accuracy 78.12% running avg accuracy 83.33%\n",
            "[epoch 70][aug 0/2][700/781] loss 0.4522 accuracy 82.81% running avg accuracy 83.28%\n",
            "\n",
            "[epoch 70] accuracy on test data: 80.52%\n",
            "\n",
            "\n",
            "epoch 71 learning rate 0.100000\n",
            "\n",
            "[epoch 71][aug 0/2][0/781] loss 0.6030 accuracy 81.25% running avg accuracy 83.08%\n",
            "[epoch 71][aug 0/2][100/781] loss 0.4439 accuracy 78.12% running avg accuracy 82.58%\n",
            "[epoch 71][aug 0/2][200/781] loss 0.3919 accuracy 73.44% running avg accuracy 81.67%\n",
            "[epoch 71][aug 0/2][300/781] loss 0.6258 accuracy 76.56% running avg accuracy 81.16%\n",
            "[epoch 71][aug 0/2][400/781] loss 0.3700 accuracy 81.25% running avg accuracy 81.17%\n",
            "[epoch 71][aug 0/2][500/781] loss 0.3302 accuracy 87.50% running avg accuracy 81.80%\n",
            "[epoch 71][aug 0/2][600/781] loss 0.4970 accuracy 71.88% running avg accuracy 80.81%\n",
            "[epoch 71][aug 0/2][700/781] loss 0.5121 accuracy 78.12% running avg accuracy 80.54%\n",
            "\n",
            "[epoch 71] accuracy on test data: 78.26%\n",
            "\n",
            "\n",
            "epoch 72 learning rate 0.100000\n",
            "\n",
            "[epoch 72][aug 0/2][0/781] loss 0.5593 accuracy 90.62% running avg accuracy 81.55%\n",
            "[epoch 72][aug 0/2][100/781] loss 0.3537 accuracy 84.38% running avg accuracy 81.83%\n",
            "[epoch 72][aug 0/2][200/781] loss 0.4665 accuracy 75.00% running avg accuracy 81.15%\n",
            "[epoch 72][aug 0/2][300/781] loss 0.4818 accuracy 73.44% running avg accuracy 80.38%\n",
            "[epoch 72][aug 0/2][400/781] loss 0.5399 accuracy 85.94% running avg accuracy 80.93%\n",
            "[epoch 72][aug 0/2][500/781] loss 0.2614 accuracy 81.25% running avg accuracy 80.96%\n",
            "[epoch 72][aug 0/2][600/781] loss 0.5456 accuracy 84.38% running avg accuracy 81.31%\n",
            "[epoch 72][aug 0/2][700/781] loss 0.3963 accuracy 92.19% running avg accuracy 82.39%\n",
            "\n",
            "[epoch 72] accuracy on test data: 80.20%\n",
            "\n",
            "\n",
            "epoch 73 learning rate 0.100000\n",
            "\n",
            "[epoch 73][aug 0/2][0/781] loss 0.5126 accuracy 84.38% running avg accuracy 82.59%\n",
            "[epoch 73][aug 0/2][100/781] loss 0.2215 accuracy 85.94% running avg accuracy 82.93%\n",
            "[epoch 73][aug 0/2][200/781] loss 0.4489 accuracy 64.06% running avg accuracy 81.04%\n",
            "[epoch 73][aug 0/2][300/781] loss 0.8067 accuracy 76.56% running avg accuracy 80.59%\n",
            "[epoch 73][aug 0/2][400/781] loss 0.6507 accuracy 82.81% running avg accuracy 80.81%\n",
            "[epoch 73][aug 0/2][500/781] loss 0.6860 accuracy 73.44% running avg accuracy 80.08%\n",
            "[epoch 73][aug 0/2][600/781] loss 0.5012 accuracy 79.69% running avg accuracy 80.04%\n",
            "[epoch 73][aug 0/2][700/781] loss 0.4944 accuracy 73.44% running avg accuracy 79.38%\n",
            "\n",
            "[epoch 73] accuracy on test data: 66.11%\n",
            "\n",
            "\n",
            "epoch 74 learning rate 0.100000\n",
            "\n",
            "[epoch 74][aug 0/2][0/781] loss 0.3576 accuracy 64.06% running avg accuracy 77.85%\n",
            "[epoch 74][aug 0/2][100/781] loss 0.3181 accuracy 84.38% running avg accuracy 78.50%\n",
            "[epoch 74][aug 0/2][200/781] loss 0.2906 accuracy 90.62% running avg accuracy 79.71%\n",
            "[epoch 74][aug 0/2][300/781] loss 0.3860 accuracy 87.50% running avg accuracy 80.49%\n",
            "[epoch 74][aug 0/2][400/781] loss 0.9949 accuracy 75.00% running avg accuracy 79.94%\n",
            "[epoch 74][aug 0/2][500/781] loss 0.3851 accuracy 87.50% running avg accuracy 80.70%\n",
            "[epoch 74][aug 0/2][600/781] loss 0.4580 accuracy 84.38% running avg accuracy 81.06%\n",
            "[epoch 74][aug 0/2][700/781] loss 0.7671 accuracy 81.25% running avg accuracy 81.08%\n",
            "\n",
            "[epoch 74] accuracy on test data: 78.09%\n",
            "\n",
            "\n",
            "epoch 75 learning rate 0.100000\n",
            "\n",
            "[epoch 75][aug 0/2][0/781] loss 0.3441 accuracy 85.94% running avg accuracy 81.57%\n",
            "[epoch 75][aug 0/2][100/781] loss 0.3518 accuracy 89.06% running avg accuracy 82.32%\n",
            "[epoch 75][aug 0/2][200/781] loss 0.4241 accuracy 92.19% running avg accuracy 83.31%\n",
            "[epoch 75][aug 0/2][300/781] loss 0.3870 accuracy 87.50% running avg accuracy 83.72%\n",
            "[epoch 75][aug 0/2][400/781] loss 0.5933 accuracy 85.94% running avg accuracy 83.95%\n",
            "[epoch 75][aug 0/2][500/781] loss 0.2819 accuracy 85.94% running avg accuracy 84.15%\n",
            "[epoch 75][aug 0/2][600/781] loss 0.7392 accuracy 79.69% running avg accuracy 83.70%\n",
            "[epoch 75][aug 0/2][700/781] loss 0.6129 accuracy 73.44% running avg accuracy 82.67%\n",
            "\n",
            "[epoch 75] accuracy on test data: 76.92%\n",
            "\n",
            "\n",
            "epoch 76 learning rate 0.100000\n",
            "\n",
            "[epoch 76][aug 0/2][0/781] loss 0.3606 accuracy 84.38% running avg accuracy 82.84%\n",
            "[epoch 76][aug 0/2][100/781] loss 0.5359 accuracy 84.38% running avg accuracy 83.00%\n",
            "[epoch 76][aug 0/2][200/781] loss 0.4909 accuracy 78.12% running avg accuracy 82.51%\n",
            "[epoch 76][aug 0/2][300/781] loss 0.7303 accuracy 79.69% running avg accuracy 82.23%\n",
            "[epoch 76][aug 0/2][400/781] loss 0.4484 accuracy 78.12% running avg accuracy 81.82%\n",
            "[epoch 76][aug 0/2][500/781] loss 0.3574 accuracy 76.56% running avg accuracy 81.29%\n",
            "[epoch 76][aug 0/2][600/781] loss 0.2951 accuracy 82.81% running avg accuracy 81.44%\n",
            "[epoch 76][aug 0/2][700/781] loss 0.4202 accuracy 82.81% running avg accuracy 81.58%\n",
            "\n",
            "[epoch 76] accuracy on test data: 78.25%\n",
            "\n",
            "\n",
            "epoch 77 learning rate 0.100000\n",
            "\n",
            "[epoch 77][aug 0/2][0/781] loss 0.3193 accuracy 90.62% running avg accuracy 82.48%\n",
            "[epoch 77][aug 0/2][100/781] loss 0.5072 accuracy 71.88% running avg accuracy 81.42%\n",
            "[epoch 77][aug 0/2][200/781] loss 0.4912 accuracy 79.69% running avg accuracy 81.25%\n",
            "[epoch 77][aug 0/2][300/781] loss 0.4640 accuracy 87.50% running avg accuracy 81.88%\n",
            "[epoch 77][aug 0/2][400/781] loss 0.4568 accuracy 87.50% running avg accuracy 82.44%\n",
            "[epoch 77][aug 0/2][500/781] loss 0.5443 accuracy 90.62% running avg accuracy 83.26%\n",
            "[epoch 77][aug 0/2][600/781] loss 0.4699 accuracy 85.94% running avg accuracy 83.52%\n",
            "[epoch 77][aug 0/2][700/781] loss 0.6208 accuracy 78.12% running avg accuracy 82.98%\n",
            "\n",
            "[epoch 77] accuracy on test data: 79.97%\n",
            "\n",
            "\n",
            "epoch 78 learning rate 0.100000\n",
            "\n",
            "[epoch 78][aug 0/2][0/781] loss 0.3818 accuracy 75.00% running avg accuracy 82.19%\n",
            "[epoch 78][aug 0/2][100/781] loss 0.4125 accuracy 81.25% running avg accuracy 82.09%\n",
            "[epoch 78][aug 0/2][200/781] loss 0.5027 accuracy 79.69% running avg accuracy 81.85%\n",
            "[epoch 78][aug 0/2][300/781] loss 0.3108 accuracy 68.75% running avg accuracy 80.54%\n",
            "[epoch 78][aug 0/2][400/781] loss 0.3879 accuracy 82.81% running avg accuracy 80.77%\n",
            "[epoch 78][aug 0/2][500/781] loss 0.5109 accuracy 73.44% running avg accuracy 80.04%\n",
            "[epoch 78][aug 0/2][600/781] loss 0.4868 accuracy 84.38% running avg accuracy 80.47%\n",
            "[epoch 78][aug 0/2][700/781] loss 0.2839 accuracy 89.06% running avg accuracy 81.33%\n",
            "\n",
            "[epoch 78] accuracy on test data: 76.38%\n",
            "\n",
            "\n",
            "epoch 79 learning rate 0.100000\n",
            "\n",
            "[epoch 79][aug 0/2][0/781] loss 0.4721 accuracy 84.38% running avg accuracy 81.63%\n",
            "[epoch 79][aug 0/2][100/781] loss 0.5679 accuracy 87.50% running avg accuracy 82.22%\n",
            "[epoch 79][aug 0/2][200/781] loss 0.3906 accuracy 82.81% running avg accuracy 82.28%\n",
            "[epoch 79][aug 0/2][300/781] loss 0.3895 accuracy 84.38% running avg accuracy 82.49%\n",
            "[epoch 79][aug 0/2][400/781] loss 0.3398 accuracy 90.62% running avg accuracy 83.30%\n",
            "[epoch 79][aug 0/2][500/781] loss 0.4087 accuracy 82.81% running avg accuracy 83.25%\n",
            "[epoch 79][aug 0/2][600/781] loss 0.4567 accuracy 75.00% running avg accuracy 82.43%\n",
            "[epoch 79][aug 0/2][700/781] loss 0.4894 accuracy 76.56% running avg accuracy 81.84%\n",
            "\n",
            "[epoch 79] accuracy on test data: 74.65%\n",
            "\n",
            "\n",
            "epoch 80 learning rate 0.100000\n",
            "\n",
            "[epoch 80][aug 0/2][0/781] loss 0.7255 accuracy 71.88% running avg accuracy 80.84%\n",
            "[epoch 80][aug 0/2][100/781] loss 0.4542 accuracy 84.38% running avg accuracy 81.20%\n",
            "[epoch 80][aug 0/2][200/781] loss 0.4213 accuracy 78.12% running avg accuracy 80.89%\n",
            "[epoch 80][aug 0/2][300/781] loss 0.5435 accuracy 85.94% running avg accuracy 81.40%\n",
            "[epoch 80][aug 0/2][400/781] loss 0.4643 accuracy 90.62% running avg accuracy 82.32%\n",
            "[epoch 80][aug 0/2][500/781] loss 0.5445 accuracy 85.94% running avg accuracy 82.68%\n",
            "[epoch 80][aug 0/2][600/781] loss 0.3668 accuracy 85.94% running avg accuracy 83.01%\n",
            "[epoch 80][aug 0/2][700/781] loss 0.6082 accuracy 87.50% running avg accuracy 83.46%\n",
            "\n",
            "[epoch 80] accuracy on test data: 80.58%\n",
            "\n",
            "\n",
            "epoch 81 learning rate 0.100000\n",
            "\n",
            "[epoch 81][aug 0/2][0/781] loss 0.3203 accuracy 90.62% running avg accuracy 84.17%\n",
            "[epoch 81][aug 0/2][100/781] loss 0.4260 accuracy 70.31% running avg accuracy 82.79%\n",
            "[epoch 81][aug 0/2][200/781] loss 0.2943 accuracy 76.56% running avg accuracy 82.16%\n",
            "[epoch 81][aug 0/2][300/781] loss 0.3811 accuracy 81.25% running avg accuracy 82.07%\n",
            "[epoch 81][aug 0/2][400/781] loss 0.3885 accuracy 78.12% running avg accuracy 81.68%\n",
            "[epoch 81][aug 0/2][500/781] loss 0.2731 accuracy 89.06% running avg accuracy 82.42%\n",
            "[epoch 81][aug 0/2][600/781] loss 0.4240 accuracy 79.69% running avg accuracy 82.14%\n",
            "[epoch 81][aug 0/2][700/781] loss 0.4963 accuracy 79.69% running avg accuracy 81.90%\n",
            "\n",
            "[epoch 81] accuracy on test data: 74.38%\n",
            "\n",
            "\n",
            "epoch 82 learning rate 0.100000\n",
            "\n",
            "[epoch 82][aug 0/2][0/781] loss 0.4174 accuracy 76.56% running avg accuracy 81.36%\n",
            "[epoch 82][aug 0/2][100/781] loss 0.4892 accuracy 85.94% running avg accuracy 81.82%\n",
            "[epoch 82][aug 0/2][200/781] loss 0.3024 accuracy 90.62% running avg accuracy 82.70%\n",
            "[epoch 82][aug 0/2][300/781] loss 0.4807 accuracy 68.75% running avg accuracy 81.31%\n",
            "[epoch 82][aug 0/2][400/781] loss 0.4792 accuracy 82.81% running avg accuracy 81.46%\n",
            "[epoch 82][aug 0/2][500/781] loss 0.4587 accuracy 81.25% running avg accuracy 81.44%\n",
            "[epoch 82][aug 0/2][600/781] loss 0.3000 accuracy 90.62% running avg accuracy 82.36%\n",
            "[epoch 82][aug 0/2][700/781] loss 0.4666 accuracy 89.06% running avg accuracy 83.03%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}