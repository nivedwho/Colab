{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyStyleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAw61y4urhRN"
      },
      "source": [
        "# **Pytorch implementation of StyleGAN**\n",
        "#### source:  https://arxiv.org/pdf/1912.04958.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRRrUaHQrhRO"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE9TnWS_rhRO"
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNx1h53yrhRP"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = (11,11)\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-L63GKl9gs4"
      },
      "source": [
        "##Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "898L4RwAS05t"
      },
      "source": [
        "from copy import deepcopy\r\n",
        "\r\n",
        "class Scaled_Act(nn.Module):\r\n",
        "    '''\r\n",
        "    Scale nonlinearity. By default it scales to retain signal variance\r\n",
        "    '''\r\n",
        "    to_str = {'Sigmoid' : 'sigmoid', 'ReLU': 'relu', 'Tanh' : 'tanh', 'LeakyReLU': 'leaky_relu'}\r\n",
        "    def __init__(self, act, scale = None):\r\n",
        "        super().__init__()\r\n",
        "        self.act = act\r\n",
        "        act_name = Scaled_Act.to_str.get(act._get_name(), act._get_name())\r\n",
        "        param = getattr(act, 'negative_slope', None)\r\n",
        "        self.scale = scale if scale else torch.nn.init.calculate_gain(act_name, param)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.scale*self.act(input)\r\n",
        "\r\n",
        "class Equal_LR:\r\n",
        "    '''\r\n",
        "    Equalized learning rate. Applies recursively to all submodules.\r\n",
        "    '''\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "\r\n",
        "    def compute_norm(module, weight):\r\n",
        "        mode = 'fan_in'\r\n",
        "        if hasattr(module, 'transposed') and module.transposed:\r\n",
        "            mode = 'fan_out'\r\n",
        "        return torch.nn.init._calculate_correct_fan(weight, mode)\r\n",
        "\r\n",
        "\r\n",
        "    def scale_weight(self, module, input):\r\n",
        "        # IDEA: maybe @property —Åonsumes less memory that creating attribute with hook\r\n",
        "        setattr(module, self.name, module.scale*module.weight_orig)\r\n",
        "\r\n",
        "\r\n",
        "    def fn(self, module):\r\n",
        "        try:\r\n",
        "            weight = getattr(module, self.name)\r\n",
        "            module.scale = 1/np.sqrt(Equal_LR.compute_norm(module, weight))\r\n",
        "            if isinstance(weight, torch.nn.Parameter):\r\n",
        "                # register new parameter -- unscaled weight\r\n",
        "                module.weight_orig = nn.Parameter(weight.clone()/module.scale)\r\n",
        "                # delete old parameter\r\n",
        "                del module._parameters[self.name]\r\n",
        "            else:\r\n",
        "                # register new buffer -- unscaled weight\r\n",
        "                module.register_buffer('weight_orig', weight.clone()/module.scale)\r\n",
        "                # delete old buffer\r\n",
        "                del module._buffers[self.name]\r\n",
        "            module.equalize = module.register_forward_pre_hook(self.scale_weight)\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "\r\n",
        "    def __call__(self, module):\r\n",
        "        new_module = deepcopy(module)\r\n",
        "        new_module.apply(self.fn)\r\n",
        "        return new_module\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def parameters_to_buffers(m):\r\n",
        "    '''\r\n",
        "    Move all parameters to buffers (non-recursive)\r\n",
        "    '''\r\n",
        "    params = m._parameters.copy()\r\n",
        "    m._parameters.clear()\r\n",
        "    for n,p in params.items():\r\n",
        "        m.register_buffer(n, p.data)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def grid(array, ncols=8):\r\n",
        "    \"\"\"\r\n",
        "    Makes grid from batch of images with shape (n_batch, height, width, channels)\r\n",
        "    \"\"\"\r\n",
        "    array = np.pad(array, [(0,0),(1,1),(1,1),(0,0)], 'constant')\r\n",
        "    nindex, height, width, intensity = array.shape\r\n",
        "    ncols = min(nindex, ncols)\r\n",
        "    nrows = (nindex+ncols-1)//ncols\r\n",
        "    r = nrows*ncols - nindex # remainder\r\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\r\n",
        "    arr = np.concatenate([array]+[np.zeros([1,height,width,intensity])]*r)\r\n",
        "    result = (arr.reshape(nrows, ncols, height, width, intensity)\r\n",
        "              .swapaxes(1,2)\r\n",
        "              .reshape(height*nrows, width*ncols, intensity))\r\n",
        "    return np.pad(result, [(1,1),(1,1),(0,0)], 'constant')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class NextDataLoader(torch.utils.data.DataLoader):\r\n",
        "    '''\r\n",
        "    Dataloader with __next__ method\r\n",
        "    '''\r\n",
        "    def __next__(self):\r\n",
        "        try:\r\n",
        "            return next(self.iterator)\r\n",
        "        except:\r\n",
        "            self.iterator = self.__iter__()\r\n",
        "            return next(self.iterator)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def to_tensor(obj, device='cuda'):\r\n",
        "    '''\r\n",
        "    Convert ndarray to tensor. Supports both batches and single objects.\r\n",
        "    '''\r\n",
        "    if obj.shape[-1] != 3 and obj.shape[-1] != 1:\r\n",
        "        obj = np.expand_dims(obj,-1)\r\n",
        "    if obj.ndim < 4:\r\n",
        "        obj = np.expand_dims(obj,0)\r\n",
        "    t = torch.tensor(np.moveaxis(obj,-1,-3), dtype=torch.float, device=device)\r\n",
        "    return t\r\n",
        "\r\n",
        "\r\n",
        "def to_img(obj):\r\n",
        "    '''\r\n",
        "    Convert tensor to ndarray. Supports both batches and single objects.\r\n",
        "    '''\r\n",
        "    array = np.moveaxis(obj.data.cpu().numpy(),-3,-1)\r\n",
        "    return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W72wVP6wyQBv"
      },
      "source": [
        "##Network functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br3i9mRHTG79"
      },
      "source": [
        "class Modulated_Conv2d(nn.Conv2d):\r\n",
        "    '''\r\n",
        "    Modulated convolution layer. Efficient implementation using grouped convolutions.\r\n",
        "    '''\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\r\n",
        "                 demodulate=True, bias=True, stride=1, padding=0, dilation=1, **kwargs):\r\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride,\r\n",
        "                         padding, dilation, groups=1,\r\n",
        "                         bias=bias, padding_mode='zeros')\r\n",
        "        self.demodulate = demodulate\r\n",
        "        # style mapping\r\n",
        "        self.style = nn.Linear(latent_size, in_channels)\r\n",
        "        # required shape might be different in transposed conv\r\n",
        "        self.s_broadcast_view = (-1,1,self.in_channels,1,1)\r\n",
        "        self.in_channels_dim = 2\r\n",
        "\r\n",
        "\r\n",
        "    def convolve(self,x,w,groups):\r\n",
        "        # bias would be added later\r\n",
        "        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, groups=groups)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, v):\r\n",
        "        N, in_channels, H, W = x.shape\r\n",
        "\r\n",
        "        # new minibatch dim: (ch dims, K, K) -> (1, ch dims, K, K)\r\n",
        "        w = self.weight.unsqueeze(0)\r\n",
        "\r\n",
        "        # compute styles: (N, C_in)\r\n",
        "        s = self.style(v) + 1\r\n",
        "\r\n",
        "        # modulate: (N, ch dims, K, K)\r\n",
        "        w = s.view(self.s_broadcast_view)*w\r\n",
        "\r\n",
        "        # demodulate\r\n",
        "        if self.demodulate:\r\n",
        "            sigma = torch.sqrt((w**2).sum(dim=[self.in_channels_dim,3,4],keepdim=True) + 1e-8)\r\n",
        "            w = w/sigma\r\n",
        "\r\n",
        "        # reshape x: (N, C_in, H, W) -> (1, N*C_in, H, W)\r\n",
        "        x = x.view(1, -1, H, W)\r\n",
        "\r\n",
        "        # reshape w: (N, C_out, C_in, K, K) -> (N*C_out, C_in, K, K) for common conv\r\n",
        "        #            (N, C_in, C_out, K, K) -> (N*C_in, C_out, K, K) for transposed conv\r\n",
        "        w = w.view(-1, w.shape[2], w.shape[3], w.shape[4])\r\n",
        "\r\n",
        "        # use groups so that each sample in minibatch has it's own conv,\r\n",
        "        # conv weights are concatenated along dim=0\r\n",
        "        out = self.convolve(x,w,N)\r\n",
        "\r\n",
        "        # reshape back to minibatch.\r\n",
        "        out = out.view(N,-1,out.shape[2],out.shape[3])\r\n",
        "\r\n",
        "        # add bias\r\n",
        "        if not self.bias is None:\r\n",
        "            out += self.bias.view(1, self.bias.shape[0], 1, 1)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Up_Mod_Conv(Modulated_Conv2d):\r\n",
        "    '''\r\n",
        "    Modulated transposed convolution layer with upsampling by some factor\r\n",
        "    '''\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\r\n",
        "                 demodulate=True, bias=True, factor=2):\r\n",
        "        assert (kernel_size % 2 == 1)\r\n",
        "        padding = (max(kernel_size-factor,0)+1)//2\r\n",
        "        super().__init__(in_channels, out_channels, kernel_size, latent_size, demodulate, bias,\r\n",
        "                         stride=factor, padding=padding)\r\n",
        "        self.output_padding = torch.nn.modules.utils._pair(2*padding - kernel_size + factor)\r\n",
        "        # transpose as expected in F.conv_transpose2d\r\n",
        "        self.weight = nn.Parameter(self.weight.transpose(0,1).contiguous())\r\n",
        "        self.transposed = True\r\n",
        "        # taking into account transposition\r\n",
        "        self.s_broadcast_view = (-1,self.in_channels,1,1,1)\r\n",
        "        self.in_channels_dim = 1\r\n",
        "\r\n",
        "    def convolve(self, x, w, groups):\r\n",
        "        return F.conv_transpose2d(x, w, None, self.stride, self.padding, self.output_padding, groups, self.dilation)\r\n",
        "\r\n",
        "\r\n",
        "class Down_Mod_Conv(Modulated_Conv2d):\r\n",
        "    '''\r\n",
        "    Modulated convolution layer with downsampling by some factor\r\n",
        "    '''\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\r\n",
        "                 demodulate=True, bias=True, factor=2):\r\n",
        "        assert (kernel_size % 2 == 1)\r\n",
        "        padding = kernel_size//2\r\n",
        "        super().__init__(in_channels, out_channels, kernel_size, latent_size, demodulate, bias,\r\n",
        "                         stride=factor, padding=padding)\r\n",
        "\r\n",
        "    def convolve(self, x, w, groups):\r\n",
        "        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, groups=groups)\r\n",
        "\r\n",
        "\r\n",
        "class Down_Conv2d(nn.Conv2d):\r\n",
        "    '''\r\n",
        "    Convolution layer with downsampling by some factor\r\n",
        "    '''\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\r\n",
        "                 bias=True, factor=2):\r\n",
        "        assert (kernel_size % 2 == 1)\r\n",
        "        padding = kernel_size//2\r\n",
        "        super().__init__(in_channels, out_channels, kernel_size, factor, padding, bias=True)\r\n",
        "\r\n",
        "    def convolve(self, x):\r\n",
        "        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\r\n",
        "\r\n",
        "\r\n",
        "class Noise(nn.Module):\r\n",
        "    '''\r\n",
        "    Add normal noise with learnable magnitude.\r\n",
        "    '''\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.noise_strength = nn.Parameter(torch.zeros(1))\r\n",
        "\r\n",
        "    def forward(self, x, input_noise=None):\r\n",
        "        if input_noise is None:\r\n",
        "            input_noise = torch.randn(x.shape[0],1,x.shape[2],x.shape[3], device=x.device)\r\n",
        "        noise = self.noise_strength*input_noise\r\n",
        "        return x + noise\r\n",
        "\r\n",
        "class Mapping(nn.Module):\r\n",
        "    '''\r\n",
        "    Mapping network. Transforms the input latent code to the disentangled latent representation.\r\n",
        "    '''\r\n",
        "    def __init__(self, n_layers, latent_size, nonlinearity, normalize=True):\r\n",
        "        super().__init__()\r\n",
        "        self.normalize = normalize\r\n",
        "        self.layers = []\r\n",
        "        for idx in range(n_layers):\r\n",
        "            layer = nn.Linear(latent_size, latent_size)\r\n",
        "            self.add_module(str(idx), layer)\r\n",
        "            self.layers.append(layer)\r\n",
        "            self.layers.append(nonlinearity)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        if self.normalize:\r\n",
        "            input = input/torch.sqrt((input**2).mean(dim=1, keepdim=True) + 1e-8)\r\n",
        "        for module in self.layers:\r\n",
        "            input = module(input)\r\n",
        "        return input\r\n",
        "\r\n",
        "\r\n",
        "class G_Block(nn.Module):\r\n",
        "    '''\r\n",
        "    Basic block for generator. Increases spatial dimensions by predefined factor.\r\n",
        "    '''\r\n",
        "    def __init__(self, in_fmaps, out_fmaps, kernel_size, latent_size, nonlinearity, factor=2, img_channels=3):\r\n",
        "        super().__init__()\r\n",
        "        inter_fmaps = (in_fmaps + out_fmaps)//2\r\n",
        "        self.upconv = Up_Mod_Conv(in_fmaps, inter_fmaps, kernel_size, latent_size,\r\n",
        "                                      factor=factor)\r\n",
        "        self.conv = Modulated_Conv2d(inter_fmaps, out_fmaps, kernel_size, latent_size,\r\n",
        "                                     padding=kernel_size//2)\r\n",
        "        self.noise = Noise()\r\n",
        "        self.noise2 = Noise()\r\n",
        "        self.to_channels = Modulated_Conv2d(out_fmaps, img_channels, kernel_size=1,\r\n",
        "                                      latent_size=latent_size, demodulate = False)\r\n",
        "        self.upsample = nn.Upsample(scale_factor=factor, mode='bilinear', align_corners=False)\r\n",
        "        self.act = nonlinearity\r\n",
        "\r\n",
        "    def forward(self, x, v, y=None, input_noises=None):\r\n",
        "        x = self.noise(self.upconv(x,v), None if (input_noises is None) else input_noises[:,0])\r\n",
        "        x = self.act(x)\r\n",
        "        x = self.noise2(self.conv(x,v), None if (input_noises is None) else input_noises[:,1])\r\n",
        "        x = self.act(x)\r\n",
        "        if not y is None:\r\n",
        "            y = self.upsample(y)\r\n",
        "        else:\r\n",
        "            y = 0\r\n",
        "        y = y + self.to_channels(x,v)\r\n",
        "        return x, y\r\n",
        "\r\n",
        "class D_Block(nn.Module):\r\n",
        "    '''\r\n",
        "    Basic block for discriminator. Decreases spatial dimensions by predefined factor.\r\n",
        "    '''\r\n",
        "    def __init__(self, in_fmaps, out_fmaps, kernel_size, nonlinearity, factor=2):\r\n",
        "        super().__init__()\r\n",
        "        inter_fmaps = (in_fmaps + out_fmaps)//2\r\n",
        "        self.conv = nn.Conv2d(in_fmaps, inter_fmaps, kernel_size, padding=kernel_size//2)\r\n",
        "        self.downconv = Down_Conv2d(inter_fmaps, out_fmaps, kernel_size, factor=factor)\r\n",
        "        self.skip = Down_Conv2d(in_fmaps, out_fmaps, kernel_size=1, factor=factor)\r\n",
        "        self.act = nonlinearity\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        t = x\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.act(x)\r\n",
        "        x = self.downconv(x)\r\n",
        "        x = self.act(x)\r\n",
        "        t = self.skip(t)\r\n",
        "        return (x + t)/ np.sqrt(2)\r\n",
        "\r\n",
        "\r\n",
        "class Minibatch_Stddev(nn.Module):\r\n",
        "    '''\r\n",
        "    Minibatch standard deviation layer.\r\n",
        "    '''\r\n",
        "    def __init__(self, group_size=4):\r\n",
        "        super().__init__()\r\n",
        "        self.group_size = group_size\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        s = x.shape\r\n",
        "        t = x.view(self.group_size, -1, s[1], s[2], s[3])\r\n",
        "        t = t - t.mean(dim=0, keepdim=True)\r\n",
        "        t = torch.sqrt((t**2).mean(dim=0) + 1e-8)\r\n",
        "        t = t.mean(dim=[1,2,3], keepdim=True) # [N/G,1,1,1]\r\n",
        "        t = t.repeat(self.group_size,1,1,1).expand(x.shape[0],1,*x.shape[2:])\r\n",
        "        return torch.cat((x,t),dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9VZEpzOTNmr"
      },
      "source": [
        "def G_logistic_ns(fake_logits):\r\n",
        "    return -F.logsigmoid(fake_logits).mean() # -log(D(G(z)))\r\n",
        "\r\n",
        "\r\n",
        "def D_logistic(real_logits, fake_logits):\r\n",
        "    return torch.mean(-F.logsigmoid(real_logits) + F.softplus(fake_logits)) # -log(D(x)) - log(1-D(G(z)))\r\n",
        "\r\n",
        "def R1_reg(real_imgs, real_logits):\r\n",
        "    '''\r\n",
        "    R1 regularization\r\n",
        "    '''\r\n",
        "    grads = torch.autograd.grad(real_logits.sum(), real_imgs, create_graph=True)[0]\r\n",
        "    return torch.mean((grads**2).sum(dim=[1,2,3]))\r\n",
        "\r\n",
        "class Path_length_loss(nn.Module):\r\n",
        "    '''\r\n",
        "    Path length regularization\r\n",
        "    '''\r\n",
        "    def __init__(self, decay=0.01):\r\n",
        "        super().__init__()\r\n",
        "        self.decay = decay\r\n",
        "        self.avg = 0\r\n",
        "\r\n",
        "    def forward(self, dlatent, gen_out):\r\n",
        "        # Compute |J*y|.\r\n",
        "        noise = torch.randn(gen_out.shape, device=gen_out.device)/np.sqrt(np.prod(gen_out.shape[2:])) #[N,Channels,H,W]\r\n",
        "        grads = torch.autograd.grad((gen_out * noise).sum(), dlatent, create_graph=True)[0]  #[N, num_layers, dlatent_size]\r\n",
        "        lengths = torch.sqrt((grads**2).mean(2).sum(1)) #[N]\r\n",
        "        # Update exp average. Lengths are detached\r\n",
        "        self.avg = self.decay*torch.mean(lengths.detach()) + (1-self.decay)*self.avg\r\n",
        "        return torch.mean((lengths - self.avg)**2)\r\n",
        "\r\n",
        "\r\n",
        "def Noise_reg(noise_maps, min_res=8):\r\n",
        "    '''\r\n",
        "    Noise maps regularization to suppress pixel correlation\r\n",
        "    '''\r\n",
        "    loss = 0\r\n",
        "    for nmap in noise_maps:\r\n",
        "        res = nmap.shape[-1]\r\n",
        "        while res > 8:\r\n",
        "            loss += ( torch.mean(nmap * nmap.roll(shifts=1, dims=-1), dim=[-1,-2])**2\r\n",
        "                    + torch.mean(nmap * nmap.roll(shifts=1, dims=-2), dim=[-1,-2])**2 ).sum()\r\n",
        "            nmap = F.avg_pool2d(nmap.squeeze(), 2)\r\n",
        "            res = res//2\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_IjKf-3rhRS"
      },
      "source": [
        "### Generator architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi4PIntQrhRS"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "                 k_size, blocks, img_channels, latent_size, n_layers, style_mixing_prob = 0.8,\n",
        "                 dlatent_avg_beta = 0.995, weights_avg_beta=0.99, **kwargs):\n",
        "        super().__init__()\n",
        "        dres = min_res*2**blocks - max_res\n",
        "        assert dres >= 0\n",
        "        # building mapping net\n",
        "        self.latent_size = latent_size\n",
        "        self.mapping = Mapping(n_layers, latent_size, act)\n",
        "        # learnable const\n",
        "        self.const = nn.Parameter(torch.randn(max_fmaps, min_res, min_res))\n",
        "        # building main layers\n",
        "        fmaps = np.linspace(max_fmaps, min_fmaps, blocks+1).astype('int')\n",
        "        self.layers = []\n",
        "        for i in range(blocks):\n",
        "            layer = G_Block(fmaps[i],fmaps[i+1], k_size, latent_size, act, img_channels=img_channels)\n",
        "            self.add_module(str(i), layer)\n",
        "            self.layers.append(layer)\n",
        "        if dres > 0:\n",
        "            self.crop = torch.nn.ZeroPad2d(-dres//2)\n",
        "        # style mixing\n",
        "        self.style_mixing_prob = style_mixing_prob \n",
        "        # running average of dlatents \n",
        "        self.dlatent_avg_beta = dlatent_avg_beta\n",
        "        self.register_buffer('dlatent_avg', torch.zeros(latent_size))\n",
        "        # running average of weights\n",
        "        self.weights_avg_beta = weights_avg_beta\n",
        "        self.Src_Net = deepcopy(self).apply(parameters_to_buffers)\n",
        "        self.Src_Net.train(False)      \n",
        "        \n",
        "        \n",
        "    # update running average of weights\n",
        "    def update_avg_weights(self):\n",
        "        params = dict(self.named_parameters())\n",
        "        buffers = dict(self.named_buffers())\n",
        "        for n,b in self.Src_Net.named_buffers():\n",
        "            try:\n",
        "                b.data.copy_(self.weights_avg_beta*b + (1-self.weights_avg_beta)*params[n])\n",
        "            except:\n",
        "                b.data.copy_(buffers[n])\n",
        "                \n",
        "    def load_avg_weights(self):\n",
        "        buffers = dict(self.Src_Net.named_buffers())\n",
        "        for n,p in self.named_parameters():\n",
        "            p.data.copy_(buffers[n])\n",
        "            \n",
        "            \n",
        "    # sample dlatents\n",
        "    def sample_dlatents(self, n):\n",
        "        v = self._sample_dlatents(n)\n",
        "        if self.training and self.style_mixing_prob > 0:\n",
        "            v = self._bcast_dlatents(v)\n",
        "            l = len(self.layers)\n",
        "            cut_off = torch.randint(l-1,())\n",
        "            v2 = self._bcast_dlatents(self._sample_dlatents(n))\n",
        "            mask = torch.empty(n, dtype=torch.bool).bernoulli_(self.style_mixing_prob).view(-1, 1) \\\n",
        "                   * (torch.arange(l)>cut_off)\n",
        "            v = torch.where(mask.unsqueeze(-1).to(device=v.device), v2, v)\n",
        "        return v\n",
        "    \n",
        "    def _sample_dlatents(self, n):\n",
        "        device = self.const.device\n",
        "        z = torch.randn(n, self.latent_size).to(device)\n",
        "        v = self.mapping(z)\n",
        "        # update dlatent average\n",
        "        if self.training:\n",
        "            self.dlatent_avg = self.dlatent_avg_beta*self.dlatent_avg + (1-self.dlatent_avg_beta)*v.data.mean(0)\n",
        "        return v\n",
        "    \n",
        "    def _bcast_dlatents(self, v):\n",
        "        # broadcast dlatents [N, dlatent_size] --> [N, num_layers, dlatent_size] \n",
        "        return v.unsqueeze(1).expand(-1, len(self.layers), -1)\n",
        "    \n",
        "    \n",
        "    # generate from dlatents and input noises (optionally)\n",
        "    def generate(self, v, input_noises=None):\n",
        "        x = self.const.expand(v.shape[0], *self.const.shape).contiguous()\n",
        "        input_noises = input_noises if input_noises else [None]*len(self.layers)\n",
        "        y = None\n",
        "        if v.ndim < 3:\n",
        "            v = self._bcast_dlatents(v)\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            x, y = layer(x,v[:,i],y, input_noises[i])  \n",
        "        if hasattr(self, 'crop'):\n",
        "            y = self.crop(y)\n",
        "        return y\n",
        "    \n",
        "    \n",
        "    # for training\n",
        "    def sample(self, n):\n",
        "        dlatents = self.sample_dlatents(n)\n",
        "        x = self.generate(dlatents)\n",
        "        return x\n",
        "            \n",
        "    \n",
        "    # for evaluation\n",
        "    def sample_images(self,n, truncation_psi=1):\n",
        "        with torch.no_grad():\n",
        "            v = self.Src_Net.sample_dlatents(n)\n",
        "            # truncation trick\n",
        "            if truncation_psi < 1:\n",
        "                v = self.dlatent_avg + truncation_psi*(v-self.dlatent_avg)\n",
        "            images = to_img(self.Src_Net.generate(v))\n",
        "        return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3N94uTDrhRS"
      },
      "source": [
        "### Discriminator architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_-algskrhRT"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "                 k_size, blocks, img_channels, dense_size=128, **kwargs):\n",
        "        super().__init__()\n",
        "        assert max_res <= min_res*2**blocks and max_res >= (min_res-1)*2**blocks\n",
        "        # building layers\n",
        "        fmaps = np.linspace(min_fmaps, max_fmaps, blocks+1).astype('int')\n",
        "        self.from_channels = nn.Conv2d(img_channels, fmaps[0], 1)\n",
        "        self.layers = []\n",
        "        for i in range(blocks):\n",
        "            layer = D_Block(fmaps[i],fmaps[i+1], k_size, act)\n",
        "            self.add_module(str(i), layer)\n",
        "            self.layers.append(layer)\n",
        "        self.minibatch_sttdev = Minibatch_Stddev()\n",
        "        self.conv = nn.Conv2d(fmaps[-1]+1,fmaps[-1], 3)\n",
        "        self.dense = nn.Linear(fmaps[-1]*(min_res-2)**2, dense_size)\n",
        "        self.output = nn.Linear(dense_size, 1)\n",
        "        self.act = act\n",
        "    \n",
        "    \n",
        "    def get_score(self, imgs):\n",
        "        x = self.act(self.from_channels(imgs))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.minibatch_sttdev(x)\n",
        "        x = self.act(self.conv(x))\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.act(self.dense(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUR6O5XrhRT"
      },
      "source": [
        "### Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAmj35rCrhRT"
      },
      "source": [
        "def train(G, D, dataset, max_iter, batch_size, \n",
        "          G_opt_args, D_opt_args, mapping_opt_args,\n",
        "          D_steps, pl_weight, r1_weight,\n",
        "          r1_interval, pl_interval, val_interval, num_workers, pl_batch_part, checkpoint=None):\n",
        "    \n",
        "    pl_batch = int(pl_batch_part*batch_size)\n",
        "    device = next(D.parameters()).device\n",
        "    Path_length_reg = Path_length_loss()\n",
        "\n",
        "    # create dataloader\n",
        "    dataloader = NextDataLoader(dataset, batch_size, num_workers=num_workers)\n",
        "    mean = dataset.transforms.transform.transforms[1].mean[0]\n",
        "    std = dataset.transforms.transform.transforms[1].std[0]\n",
        "    \n",
        "    # load state\n",
        "    if checkpoint:\n",
        "        G.load_state_dict(checkpoint['G'])\n",
        "        D.load_state_dict(checkpoint['D'])\n",
        "        Path_length_reg.avg = checkpoint['pl_loss_avg']\n",
        "    \n",
        "    # create optimizer\n",
        "    G_params = []\n",
        "    for n,m in G.named_children():\n",
        "        if n != 'mapping':\n",
        "            G_params.extend(m.parameters())\n",
        "    gen_optimizer = torch.optim.Adam([{'params': G_params},\n",
        "                                  {'params': G.mapping.parameters(), **mapping_opt_args},\n",
        "                                  {'params': G.const, **mapping_opt_args},\n",
        "                                  ], **G_opt_args)\n",
        "    disc_optimizer = torch.optim.Adam(D.parameters(), **D_opt_args)\n",
        "    \n",
        "    G.train()\n",
        "    D.train()\n",
        "    \n",
        "    for i in tqdm(range(max_iter)):\n",
        "        # discriminator update\n",
        "        for j in range(D_steps):\n",
        "            real_imgs = next(dataloader)[0].to(device)\n",
        "            real_imgs.requires_grad = True\n",
        "            fake_imgs = G.sample(real_imgs.shape[0])\n",
        "            real_scores = D.get_score(real_imgs)\n",
        "            fake_scores = D.get_score(fake_imgs)\n",
        "            loss =  D_logistic(real_scores, fake_scores)\n",
        "            if i % r1_interval == 0 and j == D_steps-1:\n",
        "                loss += r1_weight*r1_interval*R1_reg(real_imgs, real_scores)\n",
        "            real_imgs.requires_grad = False\n",
        "            disc_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            disc_optimizer.step()\n",
        "\n",
        "\n",
        "        # generator update\n",
        "        dlatent = G.sample_dlatents(batch_size)   \n",
        "        if i % pl_interval == 0:\n",
        "            # hack to compute path length loss with smaller minibatch (for reducing memory consumption)\n",
        "            dlatent_part1, dlatent_part_2 = dlatent[:pl_batch], dlatent[pl_batch:]\n",
        "            fake_imgs = G.generate(torch.cat((dlatent_part1, dlatent_part_2), 0))\n",
        "            fake_scores = D.get_score(fake_imgs)\n",
        "            loss = G_logistic_ns(fake_scores) \\\n",
        "                   + pl_weight*pl_interval*Path_length_reg(dlatent_part1, fake_imgs[:pl_batch])\n",
        "        else:\n",
        "            fake_imgs = G.generate(dlatent)\n",
        "            fake_scores = D.get_score(fake_imgs)\n",
        "            loss = G_logistic_ns(fake_scores)  \n",
        "        gen_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        gen_optimizer.step()   \n",
        "        # updating running average   \n",
        "        G.update_avg_weights()\n",
        "        \n",
        "        if i % val_interval == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            # print pictures\n",
        "            gen = G.sample_images(32)*std+mean\n",
        "            plt.imshow(grid(gen).squeeze())\n",
        "            plt.show()\n",
        "            # print prob distribution\n",
        "            plt.figure(figsize=(5,5))\n",
        "            plt.title('Generated vs real data')\n",
        "            plt.hist(torch.sigmoid(real_scores.data).cpu().numpy(), label='D(x)', alpha=0.5,range=[0,1])\n",
        "            plt.hist(torch.sigmoid(fake_scores.data).cpu().numpy(), label='D(G(z))',alpha=0.5,range=[0,1])\n",
        "            plt.legend(loc='best')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        if i % (20*val_interval) == 0:\n",
        "            torch.save({\n",
        "                        'G': G.state_dict(),\n",
        "                        'D': D.state_dict(),\n",
        "                        'pl_loss_avg': Path_length_reg.avg.item()\n",
        "                        }, 'checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAyxnoDrhRU"
      },
      "source": [
        "### Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZLnFOSyrhRU"
      },
      "source": [
        "img_channels = 1\n",
        "n_layers = 4  # number of layers in mapping from latents to dlatents\n",
        "latent_size = 160 # for simplicity dim of latent space = dim of dlatent space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciIysyNIrhRU"
      },
      "source": [
        "#### Parameters for building models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgwzC3K9rhRU"
      },
      "source": [
        "min_res = 4 # resolution from which the synthesis starts\n",
        "max_res = 28 # out resolution \n",
        "blocks = 3 # number of building blocks for both the generator and dicriminator\n",
        "k_size = 3 # convolutions kernel size\n",
        "max_fmaps = 128 # number of feature maps at the beginning of generation\n",
        "min_fmaps = 64  # number of feature maps before going to the number of channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfCTKtnHrhRV"
      },
      "source": [
        "weights_avg_beta=0.995 # beta for running average of generator weights\n",
        "act = Scaled_Act(nn.LeakyReLU(0.2)) # activation function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldp8CZfKrhRV"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "train_params = {\n",
        "    'max_iter': 50000, \n",
        "    'batch_size' : 160,\n",
        "    'G_opt_args' : {'lr' : 0.001, 'betas' : (0.1, 0.99)}, \n",
        "    'D_opt_args' : {'lr' : 0.001, 'betas' : (0, 0.99), 'eps' : 1e-08}, \n",
        "    'mapping_opt_args' : {'lr' : 1e-5}, \n",
        "    'D_steps': 1, \n",
        "    'pl_weight': 2, \n",
        "    'r1_weight': 8, \n",
        "    'pl_batch_part': 0.5,\n",
        "    'pl_interval': 4, \n",
        "    'r1_interval': 16, \n",
        "    'num_workers': 2, \n",
        "    'val_interval': 20 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsjIMISPrhRV"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekario6prhRW"
      },
      "source": [
        "G = Generator(min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "              k_size, blocks, img_channels, latent_size, n_layers, weights_avg_beta=weights_avg_beta).to(device)\n",
        "\n",
        "D = Discriminator(min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "                   k_size, blocks, img_channels).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DwdbhYcrhRW"
      },
      "source": [
        "#### Equalized learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04_p41DrhRW"
      },
      "source": [
        "G = Equal_LR('weight')(G)\n",
        "D = Equal_LR('weight')(D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMt_RIYyrhRW"
      },
      "source": [
        "#### Initialization of weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHIiPLEjrhRX"
      },
      "source": [
        "def init_weights(m):\n",
        "    if hasattr(m, 'weight_orig'):\n",
        "        torch.nn.init.normal_(m.weight_orig)\n",
        "    if hasattr(m, 'bias'):\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "        \n",
        "G.apply(init_weights)\n",
        "D.apply(init_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYvVvtblrhRX"
      },
      "source": [
        "#### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAwTNa2ErhRX"
      },
      "source": [
        "# Dataset\n",
        "mean = 0.1307\n",
        "std = 0.3081\n",
        "dataset = MNIST('data', transform=T.Compose([T.ToTensor(), T.Normalize((mean,), (std,))]), download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "iPOXOEZSrhRX"
      },
      "source": [
        "train(G, D, dataset, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMCMCTxYrhRY"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShIYU3E7rhRY"
      },
      "source": [
        "checkpoint = torch.load('checkpoint.pt')\n",
        "G.load_state_dict(checkpoint['G'])\n",
        "G.load_avg_weights()\n",
        "G.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3WlE4pZrhRY"
      },
      "source": [
        "#### Generated with truncation trick  $ \\Psi = 0.9 $ and using running average weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_B5Fm53rhRZ"
      },
      "source": [
        "plt.title('Generated')\n",
        "plt.imshow(grid(G.sample_images(32, truncation_psi=0.9)*std+mean).squeeze())\n",
        "plt.show()\n",
        "\n",
        "plt.title('Real data')\n",
        "i = np.random.randint(50000)\n",
        "real_imgs = dataset.data[i:32+i].unsqueeze(-1)\n",
        "plt.imshow(grid(real_imgs).squeeze())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9OLYp35Ktx1"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1-GgOMswnhj"
      },
      "source": [
        "G = Generator(min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "              k_size, blocks, img_channels, latent_size, n_layers, weights_avg_beta=weights_avg_beta).to(device)\n",
        "\n",
        "D = Discriminator(min_res, max_res, min_fmaps, max_fmaps, act, \n",
        "                   k_size, blocks, img_channels).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co7r5C1oKtx5"
      },
      "source": [
        "#### Equalized learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9__gb9oKtx6"
      },
      "source": [
        "G = Equal_LR('weight')(G)\n",
        "D = Equal_LR('weight')(D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5bdPEZUL8Xp"
      },
      "source": [
        "#### Initialization of weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHGyBsf8KtyD"
      },
      "source": [
        "def init_weights(m):\n",
        "    if hasattr(m, 'weight_orig'):\n",
        "        torch.nn.init.normal_(m.weight_orig)\n",
        "    if hasattr(m, 'bias'):\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "        \n",
        "G.apply(init_weights)\n",
        "D.apply(init_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrDwxRJhLu41"
      },
      "source": [
        "#### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMKWKPHvwniI"
      },
      "source": [
        "# Dataset\n",
        "mean = 0.1307\n",
        "std = 0.3081\n",
        "dataset = MNIST('data', transform=T.Compose([T.ToTensor(), T.Normalize((mean,), (std,))]), download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHn7nW9mwniX",
        "scrolled": false
      },
      "source": [
        "train(G, D, dataset, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmKz3iwsKtyK"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1qAzQAKtyL"
      },
      "source": [
        "checkpoint = torch.load('checkpoint.pt')\n",
        "G.load_state_dict(checkpoint['G'])\n",
        "G.load_avg_weights()\n",
        "G.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBtSF9uKtyP"
      },
      "source": [
        "#### Generated with truncation trick  $ \\Psi = 0.9 $ and using running average weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxTxI516KtyQ"
      },
      "source": [
        "plt.title('Generated')\n",
        "plt.imshow(grid(G.sample_images(32, truncation_psi=0.9)*std+mean).squeeze())\n",
        "plt.show()\n",
        "\n",
        "plt.title('Real data')\n",
        "i = np.random.randint(50000)\n",
        "real_imgs = dataset.data[i:32+i].unsqueeze(-1)\n",
        "plt.imshow(grid(real_imgs).squeeze())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ2U5Z3L_ReK"
      },
      "source": [
        "plt.title('Generated')\n",
        "#Compare affects of truncation value\n",
        "plt.imshow(grid(G.sample_images(128, truncation_psi=0.9)*std+mean, ncols=12).squeeze())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}